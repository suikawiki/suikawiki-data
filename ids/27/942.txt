[23] 
[[文字列]]であるはずの[[バイト列]]からその[[文字コード]] ([[文字符号化]])
を決定するには、
決め打ち (例: [[UTF-8]] 固定)、
[[メタ情報]] (例: [CODE[charset]] [[引数]]) 利用、
[[バイト列]]自体からの推定など、
いろいろな手法があります。

[24] 
推定手法やそれらの組合せは不確実性を伴うものの、現実には非常に広範囲かつ頻繁に用いられています。

* 文字コードの決定

[92] 
[DFN[文字コードの決定]]は、
[[バイト列]]とそれに関係する一連の情報から、その[[バイト列]]の解釈に使う[[文字符号化]]を決定する操作です。

[93] 
[[ファイル形式]]、[[転送プロトコル]]、[[プラットフォーム]]、
各種[[文字コード]]体系、その他慣習や互換性等が絡んだ複雑な問題です。

[94] 
それぞれによっていろいろな規定や実装戦略がありますが、次のように一般化できます。

[FIG(steps)[ [95] [[文字コードの決定]]

= [96] 決定的指定
= [97] [CN[BOM]]
= [98] 上書き指定
= [99] [[転送プロトコル]]による指定
= [100] [[ファイル形式]]依存の指定の検知
= [101] [[環境符号化]]の継承
= [102] [[バイト列]]等からの推定
= [103] [[プラットフォーム]]設定に基づく既定値
= [104] 最終既定値

]FIG]

[114] 
通常は[[符号化]]を1つ決定することがこの[[手順群]]の目的ですが、
[[文字コード指定メニュー]]の推奨候補の選出のように、
いくつも[[符号化]]の候補を抽出するのが良い場面もあります。

** ファイル形式の判定

[106] 
当該[[バイト列]]がどのような性格で、どのような[[ファイル形式]]や[[データ形式]]なのかがわかれば、
[[文字コード]]の決定の処理が限定されることがあります。

[107] 
当該[[ファイル形式]]等に決定方法の規定があれば、それに従うことになります。

[25] 
そうでなくても内容がある程度限定される場合は、それを前提とした検出手法を採用できます。

-*-*-

[108] 
場合によっては[[ファイル形式]]の検出と[[文字コード]]の決定が同時に処理されることがあります。
[SEE[ [[sniffing]] ]]

[109] 
[[エディター]]で[[テキストファイル]]を開く場合など、
特定の[[ファイル形式]]であるとは判明していないものの、
特定の[[ファイル形式]]の特徴をも[[文字コード]]の判定に活用できる場合があります。

-*-*-

[105] [[Web]] の場合については [[encoding sniffing algorithm]] を参照。

[56] それ以外の[[ファイル形式]]依存の方法については [[charset sniffing]] も参照。

** 明示的な指定

[110] 
[[利用者]]が[[文字符号化]]を明示的に指定する手段が提供されることがあります。
[SEE[ [[文字コード指定メニュー]] ]]

[111] 
通常はこれが最優先されるべきですが、[[セキュリティー]]等の理由で好ましくないとされる場合もあります。

-*-*-

[112] 
[[CLI]] の[[コマンドラインオプション]]や [[API]] の[[引数]]など[[プログラム]]の実行者が明示的に指定する手段が提供されることがあります。

[113] 
こうした方法の指定が最優先されるべきか、他の指定を優先するべきかは、時と場合によります。
[CITE[XHR]] の [[override charset]] が [CN[BOM]] よりは優先されないなど、
他の指定が優先されることもあります。

-*-*-

[115] 
[[ファイル形式]]によって確定的な[[符号化]]を1つ選べることがあります。

[EG[
[117] 
例えば[[ファイル形式]]が [[WebVTT]] と確定しているなら、
[[文字コード]]は [[UTF-8]] と断定できます。
]EG]

;; [116] [[エディター]]で[[テキストファイル]]として開く場合のように、
[[ファイル形式]]に基づく確定的な決定は[[利用者]]の指定で上書きできることが望ましい場合があります。


** 転送プロトコルによる指定

[118] 
[[HTTPヘッダー]]
や
[[MIMEヘッダー]]の 
[CODE[Content-Type:]] 
に指定された
[[MIME型]]が[[文字コード]]を表す
[CODE[charset]]
[[引数]]を伴っている場合、
これが[[転送プロトコル]]による指定に当たります。

[119] 
その指定方法や解釈方法には[[MIME型]]ごとに少しずつ違いがあるので注意も必要です。
[SEE[ [[charset]] ]]

[120] 
[[Web]] では [[MIME型]]による規定の違いは必ずしも尊重されず、ほぼ一律に
([[MIME charset]] ではなく) [CITE[Encoding Standard]] の[[符号化ラベル]]に読み替えられて解釈されています。
[SEE[ [[encoding sniffing algorithm]], [[x-user-defined]] ]]

[121] 
[[MIME]] や [[HTTP]] は [CODE[charset]] の[[既定値]]を [CODE[US-ASCII]] や 
[CODE[ISO-8859-1]] とする規定を持っていましたが、
実情とまったく一致しておらず完全に無視されてきた歴史を持ちます。
[SEE[ [[charset]] ]]
[CODE[charset]] の不存在を [[HTTP]] や [[MIME]] の[[文字コード]]の暗黙的指定とみなすべきではありません。

[122] 
[[HTTPサーバー]]は [CODE[ISO-8859-1]] や [CODE[UTF-8]] やその他各地域の一般的な[[文字コード]]を機械的に
[CODE[charset]] として指定することがあります。
こうした機械的な指定は実態と乖離していることがしばしばあります。
[SEE[ [[Webブラウザーによる文字コード判定の失敗事例集]] ]]

[123] 
機械的な指定と[[著者]]による意図的な指定を区別するのは困難であり、
原則的には盲信することとなりますから、
[[文字コード指定メニュー]]などそれを手動で上書きできる機能が必須となります。

** 環境からの継承

[124] 
[[フレーム]]としての[[埋め込み]]や [[HTML]] から [[CSS]] や [[JavaScript]] 
の参照のように、「外側」からの指定が「内側」で使えることがあります。
[SEE[ [[環境符号化]] ]]


** [CN[BOM]]

[1] 
[[Web]] では[[歴史的事情]]により [CN[BOM]] の存在がかなり重視されています。
[SEE[ [[encoding sniffing algorithm]] ]]

[57] 
[CN[BOM]] に対応した仕様や実装でも、どの[[文字符号化]]の [CN[BOM]] を検知するかはかなりブレがあります。
現在の [[Web]] は [[UTF-16]] と [[UTF-8]] に限定しています。
過去の [[Web]] や [[Web]] 以外の実装はそれ以外にもいろいろなものに対応していたり、
いなかったりします。

[132] 
[CN[BOM]]
による検知は常に適用できるものではなく、使わない場合もあります。
例えば[[ファイル]]全体ではなく[[プロトコル要素]]として用いられる[[文字列]]片では
[CN[BOM]] が認められていない場合が一般的であり、その場合たとえ [CN[BOM]]
のように見えたとしてもそれは本来の[[文字列]]の先頭です。
[[文字コード]]の判定には使えません。

[EG[
[133] 
[[ZIPファイル]]の[[ファイル名]]の[[文字コードの判定]]では
[CN[BOM]]
検査を行いません。
]EG]


** ファイル形式依存の方法による検知

[173] 
[[HTML]] では [CODE[<meta charset>]] が、
[[XML]] では [CODE[[[<?xml]] [SNIP[]] [[encoding=""]]]] が、
[[CSS]] では [CODE[@charset]]
が[[文字コードの指定]]の構文です。各仕様はこれを検出する方法を定めています。
[SEE[ [[encoding sniffing algorithm]] ]]
他の[[ファイル形式]]のいくつかにも似たような構文があります。
[SEE[ [[文字コードの指定]], [[テキストファイルの先頭]] ]]

[174] 
また、[[テキストエディター]]が[[文字コードの指定]]の構文を決めていることがあります。
いくつかの[[プログラミング言語]]等もこれを採用しています。
[SEE[ [CODE[-*- coding -*-]], [CODE[vim:]], [[局所変数群リスト]], [[テキストファイルの先頭]] ]]

[175] 
[[WebVTT]] の [CODE[WEBVTT]] など、[[ファイル形式]]が確定できる[[文字列]]が[[テキストファイルの先頭]]に検知できれば、
[[文字コード]]自体が明記されていなくても自動的にその[[ファイル形式]]の規定する[[文字コード]]と推定できることがあります。


** バイト列等からの推定

[125] 
[[バイト列]]に含まれる[[バイト]]を想定される[[文字コード]]の[[符号構造]]と比較したり、
[[自然言語]]の[[文字]]の出現頻度の統計データと比較したりして、
使われている[[文字コード]]を推定する手法群があります。

[126] 
仕組み上、[[文字コード]]を断定することは不可能ですが、
実用上かなり多くの場合に正確な判断を下すことが出来ます。

[127] 
[[ローカルファイル]]や古い [[Webサイト]]など、これ以外に信頼できる方法がないことも多いです。

[128] 
[[HTML]] では[[頻度解析等の手法]]と呼ばれ、大まかな枠組みのみとはいえ規定があります。
[SEE[ [[頻度解析等の手法]] ]]

[129]
[[UTF-8]] はかなり確実に判定できることが知られています。
[SEE[ [[頻度解析等の手法]] ]]

[130] 
[[ASCII文字]]のみで構成される場合、[[復号]]のみを考慮するなら [[ASCII]]
でも [[ISO-8859-1]] でも [[Windows-1252]] でも [[UTF-8]] でも [[EUC-JP]]
でもどの回答でも正解になりますが、
その後の処理を考慮すると判定不能と判断することが望ましい場合があります。
[SEE[ [[頻度解析等の手法]] ]]

[134] 
[[フォント依存符号化]]を使った [[HTML文書]]では、
[CODE[<font face>]] 
を判定の補助情報に使う必要があります。
[SEE[ [[頻度解析等の手法]] ]]

[131] 
[[バイナリーデータ]]を与えた場合に[[バイナリー]]と判定する判定器もあります。
この挙動が望ましいかどうかは時と場合によります。
既に[[バイナリーデータ]]を除外した[[テキストファイル]]のみが入力のときは、
無理にでもどれかの[[文字符号化]]と推定するか、判定不能と返す方がいいことも多いです。

*** 判定器を意識した著者による記述

[135] 
[[文字コードの判定]]を助けるため、紛らわしい他の[[文字コード]]に出現しない[[文字]]を含めたり、
当該[[文字コード]]で典型的な[[文字]]を最初の方に含めたりする技法が使われることがあります。

[137] 
[[文字コード]]が乱立しながら[[頻度解析等の手法]]が未発達だった[[平成時代]]初期の
[[Web]] でよく用いられました。[[日本]]など乱立が著しかった地域に多く見られます。


[FIG(quote)[
[FIGCAPTION[
[136] 
[CITE[TOPICS - VC]], [TIME[2024-08-19T09:02:08.000Z]], [TIME[1998-01-31T16:05:42.656Z]] <https://web.archive.org/web/19980131160510fw_/http://www.villagecenter.co.jp/cgi-bin/contents.cgi?0=TOPICS>
]FIGCAPTION]

>
[PRE[
<body bgcolor="black" text="white" link="yellow" vlink="#FF8080">
<!--
あいうえおかきくけこさしすせそたちつてと
IEが EUC を認識しないので、その対策です。(^_^;
-->
]PRE]

]FIG]


*** 判定器が必要な場面


[FIG(middle list)[ [26] [[文字コードの判定]]の[[応用]]

- [[encoding sniffing algorithm]]
- [[テキストファイル]]の表示
-- [[テキストエディター]]の「開く」処理
- [[ZIPファイル]]の[[ファイル名]]
- [[RFC 822メッセージ]] ([[MIME]] 以前) 
- [[query parameter]] や[[フォームデータ]]の[[復号]]
- [[CSV]]
- [[IRC]]
- [[SRT]]
- [[ID3]]
- [[T[SUB[E]]X]]
- [[QRコード]]

]FIG]

** 決定に使う入力バイト列の長さと範囲

[SEE[ [[資源ヘッダー]], [[sniffing]], [[encoding sniffing algorithm]] ]]


* 出所とロケール情報による推測

[138] 
判定したい[[バイト列]]の出所 (例えば取得に使った [[URL]] の [[TLD]])
や関係する[[ロケール]]系の情報が[[文字コードの決定]]に使われることがあります。

[86] 
利用し得る情報の例:

- [158] [[バイト列]]の取得に使った情報
-- [139] 取得を始めるために使った [[URL]]
-- [161] [[リンク元]]の [[URL]]
-- [159] [[リンク元]]の[[言語情報]]
-- [160] [[リンク元]]の[[文字コード]]情報 ([[環境符号化]]ほど信用できないもの)
- [146] [[バイト列]]に付随するメタ情報
-- [142] 実際の取得に使った [[URL]] (c.f. [[リダイレクト]]) や[[ファイル名]]
-- [165] [CODE[From:]] の[[メールアドレス]]
-- [166] [CODE[Newsgroups:]] の[[ニュースグループ]]
-- [167] [[IRCサーバー]]の[[ドメイン名]]
-- [168] [[IRC]] の[[チャンネル]]
-- [140] [CODE[Content-Location:]]
-- [147] [CODE[Content-Language:]]
-- [151] [[書庫ファイル]]の格納[[ファイル]]の [[OS]] 情報
-- [152] [[書庫ファイル]]の格納[[ファイル]]の作成[[アプリケーション]]情報
-- [162] 兄弟[[バイト列]]の情報
--- [163] 同じ[[書庫ファイル]]の他の[[ファイル]]の[[ファイル名]]とその[[文字コード]]
--- [164] [[RFC 822メッセージ]]の[[ヘッダー]]と[[本体]]の[[文字コード]]
- [153] 利用環境に関する情報
-- [154] [[Webブラウザー]]の[[言語]]設定
-- [155] [[プラットフォーム]]の[[ロケール]]設定
--- [156] [[POSIXロケール]]
--- [157] [[ANSIコードページ]], [[OEMコードページ]]

[141] [[URL]] や[[ファイル名]]や[[ドメイン名]]から利用できる情報の例:

- [143] [CODE[file:]] かどうか
- [145] [[TLD]]
- [148] [[URL]] の[[先頭一致]]
- [149] [[言語]]を表す[[拡張子]]
- [150] [[文字コード]]を表す[[拡張子]]

[169] 
利用方法:

- [170] [[文字コード指定メニュー]]の優先表示選択肢の絞り込み
- [171] [[頻度解析等の手法]]の候補の絞り込みや重みの割当
- [172] 他のどの方法でも決定できないときの既定値の選択


** TLD の利用

[39] >>38 : [CITE[[[chardetng]]]] の [[TLD]] と動作モードの関係性

[REFS[

- [38] 
[CITE@en[tld.rs - source]], [TIME[2025-05-19T13:31:23.000Z]] <https://docs.rs/chardetng/latest/src/chardetng/tld.rs.html>
- [43] 
[CITE@en[compact_enc_det/compact_enc_det/compact_enc_det.cc at master · google/compact_enc_det · GitHub]], [TIME[2025-05-19T15:36:17.000Z]] <https://github.com/google/compact_enc_det/blob/master/compact_enc_det/compact_enc_det.cc#L2059>


]REFS]


** ロケールの利用

[87] [[ロケール]]が判定のヒントに使われることがあります。
[SEE[ [[ZIPファイルの文字コード]] ]]

-*-*-

[178] 
[[HTML]] や[[テキストファイル]]の [[navigate]] では、
他の方法で決められないときの[[既定値]]が[[ロケール]]依存となっています。
[SRC[>>177]]

[179] 
より正確に言えば、[[実装定義]]または[[利用者]]指定の既定の文字符号化とすると定められています。
[SRC[>>177]]

[180] 
制御された環境や文書の[[符号化]]を予め決められる環境では、 [[UTF-8]]
を[[既定値]]とするのが[RUBYB[よい][suggested]]とされます。
例えば新しい[[ネットワーク]]の専用の[[利用者エージェント]]ではそうできると述べられています。
[SRC[>>177]]

;; [181] 具体的にそのような事例があるのかは不明です。
[[仕様書]]としては可能性を狭めないために「新しいネットワーク」
のようなものを想定しているのでしょうが、
現実的にそうしたものが大々的に導入される機会があるかは不透明です。
(例えば [[HTTPS]] や [[HTTP/2]] への移行でも、[[サーバー]]と[[内容]]は従来のままなので、
切り替えの機会とはできなかったわけで。)
特定の[[イントラネット]]や新しい種類の端末の専用ネットワークでも、
わざわざ既定値を変えるための設定や実装の変更よりは
[[HTTP]] [CODE[charset]] の指定を徹底させる方向性の方が楽そうで。

[182] 
それ以外の環境に対しては、
[[利用者]]の[[ロケール]]が[[利用者]]がよく見る[[Webページ]]の[[自然言語]]や[[符号化]]と相関があると考えられるため、
[[ロケール]]に[RUBYB[典型的には依存][typically dependent]]して既定値が定まるとされ、
当時の主要 [[Webブラウザー]]の実装状況に基づき決められた[[ロケール]]と既定の[[文字符号化]]の対応表が示されています。
[SRC[>>177]]

[REFS[

- [177] 
[CITE@en-US-x-hixie[HTML Standard]], [TIME[2025-11-04T10:59:41.000Z]], [TIME[2025-11-09T05:55:50.836Z]] <https://html.spec.whatwg.org/#determining-the-character-encoding>

]REFS]

* 符号構造や出現頻度などによる総合的な推測

[2] 
任意の[[テキストデータ]]の[[文字コード]]の判定には、
[[文字コード]]の[[バイト]]の[[範囲]]や、
出現[[文字]]の頻度・確率の情報が使われています。

[58] 
[[平成時代]]中頃までの古典的な方法では、
[[文字符号化]]によって[[符号]]の構造が異なることを利用し、
ある[[文字コード]]体系で出現する[[符号]]かそうでないかという構造的知識を主に使っていました。
しかしこの方法単独では[[符号]]構造が重複する領域で互いの区別が付きづらく、
あまり精度が上げられませんでした。
ただ、実装が容易ではあるので、幅広く用いられましたし、現在でも使われることは珍しくありません。

[EG[

[59] 例えば[[シフトJIS]]と[[日本語EUC]]は第1バイトに使われる[[バイト]]、
第2バイトに使われる[[バイト]]の範囲がそれぞれ違っていますので、
その範囲に収まるかによってどちらか判断できることが多いです。
しかし完全に重なる部分もあるため、そのような[[符号]]ばかりだと正しく判定できません。

[60] 
また、[[半角カタカナ]]を利用すると両者の重なる領域が著しく増えるため、
誤判定が多くなり、頻繁に[[半角カタカナ]]の[[文字化け]]を目にすることになります。
これは[[半角カタカナ]]が嫌われる大きな要因の1つにもなっていました。

]EG]





[5] 
特に[[日本]]と[[キリル文字]]圏では、複数の[[文字コード]]が同程度に広く使われていたために[[自動判定]]が重宝されていました。



** 実装

*** 出現頻度等による実装

[176] 出現頻度等による実装:

[REFS[

- [35] [CITE[[[UniversalCharDet]]]] の系譜
- [36] 
[CITE@en[[[GitHub]] - chomechome/charamel: 🌏 Truly Universal Encoding Detection in Python 🌎]], [TIME[2025-05-19T12:46:16.000Z]] <https://github.com/chomechome/charamel>
- [37] 
[CITE@en[GitHub - jawah/charset_normalizer: Truly universal encoding detector in pure Python]], [TIME[2025-05-19T12:51:46.000Z]] <https://github.com/jawah/charset_normalizer>
- [40] 
[CITE@en-US[Charset Detection | ICU Documentation]], [TIME[2025-04-15T18:50:10.000Z]], [TIME[2025-05-19T13:56:04.594Z]] <https://unicode-org.github.io/icu/userguide/conversion/detection.html>
- [41] 
[CITE@en[GitHub - hsivonen/shift_or_euc: Detects among the Japanese legacy encodings]], [TIME[2025-05-19T13:59:56.000Z]] <https://github.com/hsivonen/shift_or_euc>
-- [62] 
[[日本語]]系[[文字コード]]の判定
-
[91] [CITE@en[google/compact_enc_det: compact_enc_det - Compact Encoding Detection]]
([TIME[2016-07-30 15:05:47 +09:00]])
<https://github.com/google/compact_enc_det>
-- [42] 
[CITE@en[GitHub - google/compact_enc_det: compact_enc_det - Compact Encoding Detection]], [TIME[2025-05-19T14:08:52.000Z]] <https://github.com/google/compact_enc_det/>
-
[45] 
[CITE@en-US[Encode::Guess::Educated - do something - metacpan.org]], [TIME[2025-05-20T15:01:04.000Z]] <https://metacpan.org/pod/Encode::Guess::Educated>
-[81] 
[CITE@en[GitHub - vlm/zip-fix-filename-encoding: Fix cyrillic character encoding of filenames inside zip archives]], [TIME[2025-05-16T10:03:38.000Z]] <https://github.com/vlm/zip-fix-filename-encoding>
-- [144] 
[CITE@en[zip-fix-filename-encoding/src/runzip.c at master · vlm/zip-fix-filename-encoding · GitHub]], [TIME[2025-05-24T14:05:58.000Z]] <https://github.com/vlm/zip-fix-filename-encoding/blob/master/src/runzip.c>
-- [61] 
[[キリル文字]]系[[文字コード]]の判定
- [84] [CITE[Wayback Machine]], [TIME[2025-06-02T11:58:14.000Z]] <https://web.archive.org/web/20250601053528/https://shoshia.tripod.com/pub/webconv.zip>
-- [64] [CODE[GEO-CONV.PL]]
--- [65] [CODE[&analyze]]
---- [66] >>64 は[[ジョージア文字]]の変換器ですが、その改造元は[[キリル文字]]の変換器で、
(おそらく改変されていない)この関数は[[キリル文字]]の[[文字コード]]を出現頻度で推定するものです。
[[注釈]]によると
[[Stefan Mashkevich]]
が[TIME[1998-11-26]]に開発したものです。
KOI8, DOS866, WIN1251, ISO8859-5, MAC に対応しています。
- [67] 
[CITE@en[Universal online Cyrillic decoder - recover your texts]], [[Petko Yotov]], [TIME[2025-06-04T07:48:52.000Z]] <https://2cyr.com/decode/>
- [73] 
[CITE@en-US[Encode::Detect::CJK - A Charset Detector, optimized for EastAsia charset and website content - metacpan.org]], [TIME[2025-06-25T08:14:27.000Z]] <https://metacpan.org/pod/Encode::Detect::CJK>
- [77] 
[[チベット文字の文字コード]]にも[[チベット文字]]の[[符号]]の判定手法についてあり

]REFS]

-*-*-

[3] [[universalchardet]] は、 [[Mozilla]] が [[Webページ]]の表示のために開発したものです。
多くの[[プラットフォーム]]に移植されて使われています。

[4] 次の[[符号化]]に対応しています:

utf-8 utf-16be utf-16le iso-2022-cn big5 x-euc-tw gb18030 hz-gb-2312
iso-2022-jp shift_jis euc-jp iso-2022-kr euc-kr 
[CODE[iso-8859-5]] 
koi8-r
[CODE[windows-1251]]
x-mac-cyrillic ibm866 ibm855 iso-8859-7 
[CODE[tis-620]]
windows-1253
iso-8859-8 windows-1255 windows-1252

;;
[6] 
データだけで未実装:
[CODE[iso-8859-2]]
[CODE[windows-1250]]


-*-*-

[82] 
[DFN[[CITE[Charamel]]]]
は [[Python]] 用[[ライブラリー]]です。[[機械学習]]によって [[Python]]
が標準で対応するすべての[[符号化]]に対応したと謳っています。
[SRC[>>36]]

[83] 
実際に判定させてみると、他の判定器と比べて精度は今ひとつのようにも思われます。
その中には類縁の他の[[符号化]]と判断されたものがあり、
使用している[[文字]]次第でどちらとも判定できるので誤判定ではないと言えるものもありますが、
それらを除外しても不一致が多いように感じられます。
[[機械学習]]の方法による不透明なバイナリーデータを判定に用いているため、
改善も困難と思われます。

[85] 
付属の試験データ [SRC[>>31]]
は実際の [[Webページ]]らしきものやその他の[[テキスト]]データが含まれますが、
各[[文字コード]]には機械的に変換したものと見られます。
中には[[非ASCII文字]]が1つも含まれないデータしかない[[符号化]]もあり、
試験データとして精査されたものとは思えません。


@@
[88] 
[CITE@en[Expose apparent_encoding_confidence. by martinblech · Pull Request #1796 · psf/requests · GitHub]], [TIME[2025-10-19T13:06:53.000Z]] <https://github.com/psf/requests/pull/1796>

*** 符号構造のみによる実装


[54] [[符号構造]]のみによる実装:

[REFS[

-
[63] 
[CITE[null]], [TIME[2008-05-10T18:15:22.000Z]], [TIME[2025-05-24T14:10:44.885Z]] <http://openlab.ring.gr.jp/Jcode/Jcode.pm#:~:text=sub%20getcode>
- 
[44] 
[CITE@en-US[Encode::Guess - Guesses encoding from data - metacpan.org]], [TIME[2025-05-20T14:53:40.000Z]] <https://metacpan.org/pod/Encode::Guess>
-
[72] 
[CITE@en-US[Encode::HanDetect - Cross-encoding, cross-variant Chinese decoder - metacpan.org]], [TIME[2025-06-25T08:00:21.000Z]] <https://metacpan.org/pod/Encode::HanDetect>
--[74] [CITE@en-US[HanDetect.pm - metacpan.org]], [TIME[2003-06-27T04:53:28.000Z]], [TIME[2025-06-25T08:15:33.569Z]] <https://metacpan.org/module/Lingua::ZH::HanDetect/source>
-
[53] 
[CITE@ja[文字エンコーディング判定スクリプト - t_komuraの日記]], [TIME[2025-05-21T12:18:53.000Z]] <https://t-komura.hatenadiary.org/entry/20091220/1261305552>
-
[75] 
[CITE@en-US[Encode::Multibyte::Detect - detect multibyte encoding - metacpan.org]], [TIME[2025-06-25T08:18:04.000Z]] <https://metacpan.org/pod/Encode::Multibyte::Detect>


]REFS]

[76] [CITE[LV Homepage (in Japanese)]], [TIME[2025-06-25T14:34:54.000Z]], [TIME[2001-01-19T05:51:42.277Z]] <https://web.archive.org/web/20010119052900/http://www.mt.cs.keio.ac.jp/person/narita/lv/index_ja.html>

>現在の自動選択の方法は簡単なものです. ファイルを先頭から読み込んでいって, 8ビット目が立っている文字があった場合, 『その行』の中で euc-japan で使用される領域のみを使っていれば euc-japan, そうでなければ shift-jis です. つまり,『漢字らしきものを含む最初の一行』で判断しています. 8ビット目が立っている文字が見つからなければ いつまでも自動選択のままの状態が続き, 判断が必要になったときに判断します. shift-jis の片仮名のみを使用した場合や, 運の悪いときは, 誤って euc-japan と認識されます.


[70] 
[CITE@en[Add UTF-7 to replacement encoding list? / Encoding sniffing · Issue #68 · whatwg/encoding]], [TIME[2025-06-17T03:18:44.000Z]] <https://github.com/whatwg/encoding/issues/68>

[71] 
[CITE@en[Encoding: make it clear sniffing for UTF-8 is not acceptable by annevk · Pull Request #14455 · web-platform-tests/wpt · GitHub]], [TIME[2025-06-17T03:20:29.000Z]] <https://github.com/web-platform-tests/wpt/pull/14455>


** テストデータ

[REFS[

- [7] 
[CITE@en[gecko-dev/extensions/universalchardet/tests at master · mozilla/gecko-dev · GitHub]], [TIME[2025-05-17T09:04:18.000Z]] <https://github.com/mozilla/gecko-dev/tree/master/extensions/universalchardet/tests>
-- [8] [[MPL]] 2
- [9] [CITE@en[juniversalchardet/data at master · seratch/juniversalchardet · GitHub]], [TIME[2025-05-18T03:30:23.000Z]] <https://github.com/seratch/juniversalchardet/tree/master/data>
-- [10] [[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+
- [11] 
[CITE@en[juniversalchardet/src/test/resources at main · albfernandez/juniversalchardet · GitHub]], [TIME[2025-05-18T03:36:18.000Z]] <https://github.com/albfernandez/juniversalchardet/tree/main/src/test/resources>
-- [12] [[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+
-- [13] >>9 を含み、更に追加
- [14] 
[CITE@en[ude/src/Tests/Data at master · errepi/ude · GitHub]], [TIME[2025-05-18T03:40:02.000Z]] <https://github.com/errepi/ude/tree/master/src/Tests/Data>
-- [15] 「[[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+」、
「Wikipedia と同じ」、
「The Project Gutenberg と同じ」
が混在
-- [16] >>9 を含み、更に追加
- [17] 
[CITE@ja[test · master · uchardet / uchardet · GitLab]], [TIME[2025-05-18T04:21:47.000Z]] <https://gitlab.freedesktop.org/uchardet/uchardet/-/tree/master/test?ref_type=heads>
-- [18] [[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+
- [19] 
[CITE@en[rust-chardet/tests/data at master · thuleqaid/rust-chardet · GitHub]], [TIME[2025-05-18T05:01:25.000Z]] <https://github.com/thuleqaid/rust-chardet/tree/master/tests/data>
-- [20] [[LGPL]] 3
- [21] [CITE@en[chardet/tests at main · chardet/chardet · GitHub]], [TIME[2025-05-18T05:10:41.000Z]] <https://github.com/chardet/chardet/tree/main/tests>
-- [22] 不自由なものを含む
---
[28] 
[CITE@en[problematic licensing of /tests? · Issue #231 · chardet/chardet]], [TIME[2025-05-19T06:27:37.000Z]] <https://github.com/chardet/chardet/issues/231>
--- [27] 
[CITE@en[Documentation licensed only to non-commercial and personal use found · Issue #271 · chardet/chardet]], [TIME[2025-05-19T06:24:05.000Z]] <https://github.com/chardet/chardet/issues/271>
- [29] 
[CITE@en[GitHub - Ousret/char-dataset: Public dataset used to challenge Charset-Normalizer]], [TIME[2025-05-19T08:37:52.000Z]] <https://github.com/Ousret/char-dataset>
-- [30] ライセンス不明
- [31] 
[CITE@en[charamel/tests/fixtures at master · chomechome/charamel · GitHub]], [TIME[2025-05-19T11:59:39.000Z]] <https://github.com/chomechome/charamel/tree/master/tests/fixtures>
-- [32] [[Apache 2.0]] となっているが、出所の怪しげなファイルもある
-- [33] [[Git LFS]]
-- [34] 機械的に変換したファイルが多い?
- [46] 
[CITE@en[compact_enc_det/compact_enc_det/compact_enc_det_unittest.cc at master · google/compact_enc_det · GitHub]], [TIME[2025-05-21T06:26:33.000Z]] <https://github.com/google/compact_enc_det/blob/master/compact_enc_det/compact_enc_det_unittest.cc>
-- [47] [[Apache 2.0]]
-- [48] [[C++]] ソースコードに埋め込まれている


]REFS]


* 関連

[55] [[文字コード選択メニュー]]

* メモ


[80] 
[CITE@en[How to reliably guess the encoding between MacRoman, CP1252, Latin1, UTF-8, and ASCII - Stack Overflow]], [TIME[2025-10-18T08:36:44.000Z]] <https://stackoverflow.com/questions/4198804/how-to-reliably-guess-the-encoding-between-macroman-cp1252-latin1-utf-8-and>


- [51] 
[CITE@ja[mb_detect_encoding() は文字コード判定として使用できるか(その1) - t_komuraの日記]], [TIME[2025-05-21T12:15:39.000Z]] <https://t-komura.hatenadiary.org/entry/20090615/1245078430>
-- [50] 
[CITE@ja[mb_detect_encoding() は文字コードの妥当性検証として使用できるか(その2) - t_komuraの日記]], [TIME[2025-05-21T12:15:09.000Z]] <https://t-komura.hatenadiary.org/entry/20090621/1245595484>
-- [52] 
[CITE@ja[mb_detect_encoding() は文字コードの妥当性検証として使用できるか(その3) - t_komuraの日記]], [TIME[2025-05-21T12:17:54.000Z]] <https://t-komura.hatenadiary.org/entry/20090705/1246802467>





[49] [CITE@en['''['''ptexenc''']''' 入力ファイルの文字コードの自動判定 · Issue #142 · texjporg/tex-jp-build]], [TIME[2025-05-21T12:13:06.000Z]] <https://github.com/texjporg/tex-jp-build/issues/142>



- [68] [CITE@en-US[Encode::Detect::Upload - Attempt to guess user's locale encoding from IP, HTTP_ACCEPT_LANGUAGE and HTTP_USER_AGENT - metacpan.org]], [TIME[2025-06-16T10:29:46.000Z]] <https://metacpan.org/pod/Encode::Detect::Upload>

[69] >>68 [CODE[Accept-Language:]], [CODE[User-Agent:]] ([[OS]]), 
アクセス者の [[IPアドレス]]を使って[[文字符号化]]を推定する。
平成25年。



[78] 
対応している[[符号化]]を順番に試してエラーにならなかったものを採用するという実装を「文字コードの判定」だと称しているものがたまにあります。
このような方法は[[符号構造]]がまったく違う[[符号]]の区別になら使えますが、
多くの[[8ビット符号]]の区別が不可能です。

;; [79] [[符号構造]]が限定される場合なら、その限定される特徴で判定したほうが高速かつ確実なことが多いので、
この手法が役に立つことはほぼないといっていいでしょう。




[89] [CITE@en[Sub-Zero.bundle/Contents/Libraries/Shared/subliminal_patch/subtitle.py at master · pannal/Sub-Zero.bundle · GitHub]], [TIME[2025-10-21T15:34:46.000Z]] <https://github.com/pannal/Sub-Zero.bundle/blob/master/Contents/Libraries/Shared/subliminal_patch/subtitle.py>


[90] 
>>89 [[BOM]] → 言語設定から決めた後補を順に試してエラーにならなければ採用 → 判定器。
最初から判定器を使えばいいようにも思われるが、判定器の精度が不十分なので言語依存の決定が増やされてきた経緯のようで。

