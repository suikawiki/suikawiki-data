[23] 
[[文字列]]であるはずの[[バイト列]]からその[[文字コード]] ([[文字符号化]])
を決定するには、
決め打ち (例: [[UTF-8]] 固定)、
[[メタ情報]] (例: [CODE[charset]] [[引数]]) 利用、
[[バイト列]]自体からの推定など、
いろいろな手法があります。

[24] 
推定手法やそれらの組合せは不確実性を伴うものの、現実には非常に広範囲かつ頻繁に用いられています。

* 文字コードの決定

[92] 
[DFN[文字コードの決定]]は、
[[バイト列]]とそれに関係する一連の情報から、その[[バイト列]]の解釈に使う[[文字符号化]]を決定する操作です。

[93] 
[[ファイル形式]]、[[転送プロトコル]]、[[プラットフォーム]]、
各種[[文字コード]]体系、その他慣習や互換性等が絡んだ複雑な問題です。

[94] 
それぞれによっていろいろな規定や実装戦略がありますが、次のように一般化できます。

[FIG(steps)[ [95] [[文字コードの決定]]

= [96] 決定的指定
= [97] [CN[BOM]]
= [98] 上書き指定
= [99] [[転送プロトコル]]による指定
= [100] [[ファイル形式]]依存の指定の検知
= [101] [[環境符号化]]の継承
= [102] [[バイト列]]等からの推定
= [103] [[プラットフォーム]]設定に基づく既定値
= [104] 最終既定値

]FIG]

[114] 
通常は[[符号化]]を1つ決定することがこの[[手順群]]の目的ですが、
[[文字コード指定メニュー]]の推奨候補の選出のように、
いくつも[[符号化]]の候補を抽出するのが良い場面もあります。

** ファイル形式の判定

[106] 
当該[[バイト列]]がどのような性格で、どのような[[ファイル形式]]や[[データ形式]]なのかがわかれば、
[[文字コード]]の決定の処理が限定されることがあります。

[107] 
当該[[ファイル形式]]等に決定方法の規定があれば、それに従うことになります。

[25] 
そうでなくても内容がある程度限定される場合は、それを前提とした検出手法を採用できます。

-*-*-

[108] 
場合によっては[[ファイル形式]]の検出と[[文字コード]]の決定が同時に処理されることがあります。
[SEE[ [[sniffing]] ]]

[109] 
[[エディター]]で[[テキストファイル]]を開く場合など、
特定の[[ファイル形式]]であるとは判明していないものの、
特定の[[ファイル形式]]の特徴をも[[文字コード]]の判定に活用できる場合があります。

-*-*-

[105] [[Web]] の場合については [[encoding sniffing algorithm]] を参照。

[56] それ以外の[[ファイル形式]]依存の方法については [[charset sniffing]] も参照。

** 明示的な指定

[110] 
[[利用者]]が[[文字符号化]]を明示的に指定する手段が提供されることがあります。
[SEE[ [[文字コード指定メニュー]] ]]

[111] 
通常はこれが最優先されるべきですが、[[セキュリティー]]等の理由で好ましくないとされる場合もあります。

-*-*-

[112] 
[[CLI]] の[[コマンドラインオプション]]や [[API]] の[[引数]]など[[プログラム]]の実行者が明示的に指定する手段が提供されることがあります。

[113] 
こうした方法の指定が最優先されるべきか、他の指定を優先するべきかは、時と場合によります。
[CITE[XHR]] の [[override charset]] が [CN[BOM]] よりは優先されないなど、
他の指定が優先されることもあります。

-*-*-

[115] 
[[ファイル形式]]によって確定的な[[符号化]]を1つ選べることがあります。

[EG[
[117] 
例えば[[ファイル形式]]が [[WebVTT]] と確定しているなら、
[[文字コード]]は [[UTF-8]] と断定できます。
]EG]

;; [116] [[エディター]]で[[テキストファイル]]として開く場合のように、
[[ファイル形式]]に基づく確定的な決定は[[利用者]]の指定で上書きできることが望ましい場合があります。


** 転送プロトコルによる指定

[118] 
[[HTTPヘッダー]]
や
[[MIMEヘッダー]]の 
[CODE[Content-Type:]] 
に指定された
[[MIME型]]が[[文字コード]]を表す
[CODE[charset]]
[[引数]]を伴っている場合、
これが[[転送プロトコル]]による指定に当たります。

[119] 
その指定方法や解釈方法には[[MIME型]]ごとに少しずつ違いがあるので注意も必要です。
[SEE[ [[charset]] ]]

[120] 
[[Web]] では [[MIME型]]による規定の違いは必ずしも尊重されず、ほぼ一律に
([[MIME charset]] ではなく) [CITE[Encoding Standard]] の[[符号化ラベル]]に読み替えられて解釈されています。
[SEE[ [[encoding sniffing algorithm]], [[x-user-defined]] ]]

[121] 
[[MIME]] や [[HTTP]] は [CODE[charset]] の[[既定値]]を [CODE[US-ASCII]] や 
[CODE[ISO-8859-1]] とする規定を持っていましたが、
実情とまったく一致しておらず完全に無視されてきた歴史を持ちます。
[SEE[ [[charset]] ]]
[CODE[charset]] の不存在を [[HTTP]] や [[MIME]] の[[文字コード]]の暗黙的指定とみなすべきではありません。

[122] 
[[HTTPサーバー]]は [CODE[ISO-8859-1]] や [CODE[UTF-8]] やその他各地域の一般的な[[文字コード]]を機械的に
[CODE[charset]] として指定することがあります。
こうした機械的な指定は実態と乖離していることがしばしばあります。
[SEE[ [[Webブラウザーによる文字コード判定の失敗事例集]] ]]

[123] 
機械的な指定と[[著者]]による意図的な指定を区別するのは困難であり、
原則的には盲信することとなりますから、
[[文字コード指定メニュー]]などそれを手動で上書きできる機能が必須となります。

** 指定の読み替え

[206] 
[[文字コードの指定]]には色々な表現法があります。また、それぞれに複雑な事情が色々あります。
指定された[[文字コード]]の名前等はそのまま使うのではなく、適宜の読み替えが必要になります。

[207] 
[[HTML]] の [[prescan]] では、 [[ASCII]] 系の[[文字コード]]であるにも関わらず
[[UTF-16]] 系の[[文字コード]]が指定されたとき、これを [[UTF-8]]
に読み替えることになっています。
[SEE[ [[prescan]], [[UTF-16]], [[符号化ラベル]] ]]

[208] 
[[x-user-defined]] は歴史的理由により [[Windows-1252]]
に読み替えられることがあります。
[SEE[ [[x-user-defined]], [[prescan]] ]]

[209] 
[[Web]] では同じ[[符号化]]にいろいろな[[符号化ラベル]]があります。
本来は異なる[[文字コード]]を指していた[[符号化ラベル]]が、
歴史的理由によって統合されている場合が多々あります。

[210] 
同じ[[文字コード]]名でも[[インターネットメール]]と [[Web]]
とで異なる歴史的経過を辿っており、異なる読み替えが必要となる場合もあります。


** 環境からの継承

[124] 
[[フレーム]]としての[[埋め込み]]や [[HTML]] から [[CSS]] や [[JavaScript]] 
の参照のように、「外側」からの指定が「内側」で使えることがあります。
[SEE[ [[環境符号化]] ]]


** [CN[BOM]]

[1] 
[[Web]] では[[歴史的事情]]により [CN[BOM]] の存在がかなり重視されています。
[SEE[ [[encoding sniffing algorithm]] ]]

[57] 
[CN[BOM]] に対応した仕様や実装でも、どの[[文字符号化]]の [CN[BOM]] を検知するかはかなりブレがあります。
現在の [[Web]] は [[UTF-16]] と [[UTF-8]] に限定しています。
過去の [[Web]] や [[Web]] 以外の実装はそれ以外にもいろいろなものに対応していたり、
いなかったりします。

[132] 
[CN[BOM]]
による検知は常に適用できるものではなく、使わない場合もあります。
例えば[[ファイル]]全体ではなく[[プロトコル要素]]として用いられる[[文字列]]片では
[CN[BOM]] が認められていない場合が一般的であり、その場合たとえ [CN[BOM]]
のように見えたとしてもそれは本来の[[文字列]]の先頭です。
[[文字コード]]の判定には使えません。

[EG[
[133] 
[[ZIPファイル]]の[[ファイル名]]の[[文字コードの判定]]では
[CN[BOM]]
検査を行いません。
]EG]


** ファイル形式依存の方法による検知

[173] 
[[HTML]] では [CODE[<meta charset>]] が、
[[XML]] では [CODE[[[<?xml]] [SNIP[]] [[encoding=""]]]] が、
[[CSS]] では [CODE[@charset]]
が[[文字コードの指定]]の構文です。各仕様はこれを検出する方法を定めています。
[SEE[ [[encoding sniffing algorithm]] ]]
他の[[ファイル形式]]のいくつかにも似たような構文があります。
[SEE[ [[文字コードの指定]], [[テキストファイルの先頭]] ]]

[174] 
また、[[テキストエディター]]が[[文字コードの指定]]の構文を決めていることがあります。
いくつかの[[プログラミング言語]]等もこれを採用しています。
[SEE[ [CODE[-*- coding -*-]], [CODE[vim:]], [[局所変数群リスト]], [[テキストファイルの先頭]] ]]

[175] 
[[WebVTT]] の [CODE[WEBVTT]] など、[[ファイル形式]]が確定できる[[文字列]]が[[テキストファイルの先頭]]に検知できれば、
[[文字コード]]自体が明記されていなくても自動的にその[[ファイル形式]]の規定する[[文字コード]]と推定できることがあります。


[244] 
[CITE@ja[cmsd_doc_reference.pdf]], [TIME[2015-08-28T09:04:15.000Z]], [TIME[2025-11-23T04:24:09.319Z]] <https://cms.al-design.jp/downloads/EUC-JP/cmsd_doc_reference.pdf#page=69>

>
[LEFT[
[PRE[
<?php require( "cmsdesigner/include/view.php.inc" ); // encoding="euc-jp" ?>
]PRE]
]LEFT]

>
[LEFT[
は、定型文として入れてください。(「// encoding="euc-jp"」は Dreamweaver の文字化け(不具[BR[]]
合?)回避の為のおまじないです。)
]LEFT]

;; [245] [CODE[encoding=""]] の [[sniffing]] を応用した [[hack]] か?




** バイト列等からの推定

[125] 
[[バイト列]]に含まれる[[バイト]]を想定される[[文字コード]]の[[符号構造]]と比較したり、
[[自然言語]]の[[文字]]の出現頻度の統計データと比較したりして、
使われている[[文字コード]]を推定する手法群があります。

[126] 
仕組み上、[[文字コード]]を断定することは不可能ですが、
実用上かなり多くの場合に正確な判断を下すことが出来ます。

[127] 
[[ローカルファイル]]や古い [[Webサイト]]など、これ以外に信頼できる方法がないことも多いです。

[128] 
[[HTML]] では[[頻度解析等の手法]]と呼ばれ、大まかな枠組みのみとはいえ規定があります。
[SEE[ [[頻度解析等の手法]] ]]



[130] 
[[ASCII文字]]のみで構成される場合、[[復号]]のみを考慮するなら [[ASCII]]
でも [[ISO-8859-1]] でも [[Windows-1252]] でも [[UTF-8]] でも [[EUC-JP]]
でもどの回答でも正解になりますが、
その後の処理を考慮すると判定不能と判断することが望ましい場合があります。
[SEE[ [[頻度解析等の手法]] ]]

[134] 
[[フォント依存符号化]]を使った [[HTML文書]]では、
[CODE[<font face>]] 
を判定の補助情報に使う必要があります。
[SEE[ [[頻度解析等の手法]] ]]

[131] 
[[バイナリーデータ]]を与えた場合に[[バイナリー]]と判定する判定器もあります。
この挙動が望ましいかどうかは時と場合によります。
既に[[バイナリーデータ]]を除外した[[テキストファイル]]のみが入力のときは、
無理にでもどれかの[[文字符号化]]と推定するか、判定不能と返す方がいいことも多いです。

*** 判定器を意識した著者による記述

[135] 
[[文字コードの判定]]を助けるため、紛らわしい他の[[文字コード]]に出現しない[[文字]]を含めたり、
当該[[文字コード]]で典型的な[[文字]]を最初の方に含めたりする技法が使われることがあります。

[137] 
[[文字コード]]が乱立しながら[[頻度解析等の手法]]が未発達だった[[平成時代]]初期の
[[Web]] でよく用いられました。[[日本]]など乱立が著しかった地域に多く見られます。

[197] [CITE@ja[文字化けしないようにするには - とほほのWWW入門]], [TIME[2025-05-14T07:16:17.000Z]], [TIME[2025-11-19T14:30:22.643Z]] <https://www.tohoho-web.com/wwwxx005.htm#spell-character>

[253] 
[CITE[「美乳」で文字化けが直るって本当?]], [TIME[2020-08-20T23:48:40.000Z]], [TIME[2025-11-23T05:43:55.534Z]] <https://www.shtml.jp/mojibake/binew.html>

[FIG(quote)[
[FIGCAPTION[
[136] 
[CITE[TOPICS - VC]], [TIME[2024-08-19T09:02:08.000Z]], [TIME[1998-01-31T16:05:42.656Z]] <https://web.archive.org/web/19980131160510fw_/http://www.villagecenter.co.jp/cgi-bin/contents.cgi?0=TOPICS>
]FIGCAPTION]

>
[PRE[
<body bgcolor="black" text="white" link="yellow" vlink="#FF8080">
<!--
あいうえおかきくけこさしすせそたちつてと
IEが EUC を認識しないので、その対策です。(^_^;
-->
]PRE]

]FIG]

[247] 
[CITE[Yahoo! JAPAN - 文字化け対策 その他]], [TIME[2025-11-23T05:28:18.000Z]], [TIME[2003-02-02T08:51:31.798Z]] <https://web.archive.org/web/20030202085121/http://docs.yahoo.co.jp/docs/help/mojibake/sonota.html>


[232] [CITE[Yahoo! Sports]], [TIME[2025-11-23T01:50:43.000Z]], [TIME[1999-01-16T23:16:44.888Z]] <https://web.archive.org/web/19990116231624/http://yahoo.co.jp/>

>
[PRE[
<!--�-->
<!-- YTAG JDATETIME 3 START -->
<!--1999/01/17(日) 08:05:00-->
<!-- YTAG JDATETIME 3 END -->
<title>Yahoo! Sports</title>
</head>
]PRE]

;; [[EUC-JP]]

[233] [CH[�]] になっているのは [N[0xFDFE]]。空き領域ながら、
[[文字コードの判定]]の補助として使われていた [SRC[>>197]]。

[262] 
[CITE[Yahoo! JAPAN]], [TIME[2025-11-23T07:37:43.000Z]], [TIME[2007-01-01T01:46:00.472Z]] <https://web.archive.org/web/20070101014502/http://yahoo.co.jp/>

>
[PRE[
<meta http-equiv="Content-Type" content="text/html; charset=euc-jp">
<!--京-->
<title>Yahoo! JAPAN</title>
<meta name="description" content="日本最大級のポータルサイト。検索、オークション、ニュース、メール、コミュニティ、ショッピング、など80以上のサービスを展開。あなたの生活をより豊かにする「ライフ・エンジン」を目指していきます。">
]PRE]


[234] 
[CITE@ja-jp[txtやhtmlファイルが文字化けしてしまう - Microsoft Q&A]], [TIME[2025-11-23T02:02:50.000Z]] <https://learn.microsoft.com/ja-jp/answers/questions/3812530/txt-html>

>
ファイルの先頭近くに全角空白を入れておけば誤判定しにくいでしょう。

[246] 
[CITE[「美乳」で文字化けが直るって本当?]], [TIME[2020-08-20T23:48:40.000Z]], [TIME[2025-11-23T05:27:03.610Z]] <https://www.shtml.jp/mojibake/binew.html>

[263] 
[CITE[美乳?]], [TIME[2008-08-30T14:36:48.000Z]], [TIME[2025-11-23T07:40:27.037Z]] <http://www.tvt.ne.jp/~kumapooh/061106.html>


[249] [CITE[goo]], [TIME[2025-11-23T05:31:03.000Z]], [TIME[1998-11-11T19:13:55.879Z]] <https://web.archive.org/web/19981111191346/http://goo.ne.jp/>

>
[PRE[
<html>
<!-- 孝 -->
<!-- Goo Ver.2.980827 -->
<!-- Query Page -->
]PRE]

;; [[EUC-JP]]

[248] [CITE[goo]], [TIME[2025-11-23T05:30:03.000Z]], [TIME[1998-12-12T03:44:30.408Z]] <https://web.archive.org/web/19981212034346/http://www2.goo.ne.jp/>

>
[PRE[
<html>
<!-- 悌 -->
<!-- Goo Ver.2.981119 -->
<!-- Query Page -->
]PRE]

;; [[EUC-JP]]

[250] 
[CITE[goo]], [TIME[2025-11-23T05:34:16.000Z]], [TIME[2001-01-05T01:26:19.359Z]] <https://web.archive.org/web/20010105012600/http://www.goo.ne.jp/>

>
[PRE[
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<!-- 龠 -->
]PRE]

;; [[EUC-JP]]

[251] >>250 [CITE[Chrome]] は正しく [[EUC-JP]] と判定。
[CITE[Firefox]] は [[Windows-1253]] として読み込み、
しばらくして [[EUC-JP]] で再読み込み。

[252] 
[CITE@ja[goo]], [TIME[2025-11-23T05:36:59.000Z]], [TIME[2002-01-24T07:28:18.649Z]] <https://web.archive.org/web/20020124072803/http://www.goo.ne.jp/>

>
[PRE[
<!-- <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> -->
<html lang="ja">
<!-- � 龠 -->
]PRE]

;; [[EUC-JP]], [CH[�]] は [N[0xFD0xFE]]

[259] 
[CITE@ja-JP[20040209 美乳|雑記草(ざっきそう)]], [TIME[2025-11-23T07:30:11.000Z]] <https://note.com/zakkisou/n/n59402f97e0ef>

><!--美乳-->

><!--龠龠龠-->

[260] 
[CITE@ja[A memo about HTML charset information]], [TIME[2003-02-04T13:51:52.000Z]], [TIME[2025-11-23T07:32:18.330Z]] <https://www.asahi-net.or.jp/~sd5a-ucd/essay/htmlcharset.html>

>HTMLのソースを眺めていると、先頭の方に「<-- 美乳 -->」「<-- 譚 -->」などの意味不明なコメントが挿入されていることがあります。[SNIP[]]

>[SNIP[]]「美乳」や「譚」の場合、句点コードで表すと「40区94点、38区93点」「75区93点」であり、Shift_JISで符号化した場合はEUC-JPが利用しないコード、EUC-JPで符号化した場合にはShiftJISが利用しないコードになる文字なので、両者のどちらで符号化する場合でも識別子として役立つ文字であると言えます。また例えば「入口」という熟語の句点コードは「38区94点、24区93点」なので、ソースの先頭の方に記しておくのに「美乳」や「譚」よりは適した文字列でしょう。


[258] 
[CITE[文字コードの部屋 -- Web のエンコード]], [TIME[2025-11-23T07:29:08.000Z]], [TIME[2004-02-02T01:48:54.774Z]] <http://web.archive.org/web/20040202014837/http://www.mikeneko.ne.jp/~lab/kcode/web.html>

[261] 
[CITE@ja[IE7 で画面が真っ白になるページの対処方法 - drk7jp]], [TIME[2022-04-29T13:32:47.000Z]], [TIME[2025-11-23T07:35:52.874Z]] <https://www.drk7.jp/MT/archives/001163.html>


[257] 
[CITE@ja[Unicode 版美乳テーブルを探せ]], [[緇隰(くろさわ)]], [TIME[2020-02-24T06:35:00.000Z]], [TIME[2025-11-23T07:26:22.940Z]] <https://qlosawa.sakura.ne.jp/language/binyu.html>

[264] 
[CITE@ja[UTF-8 の文字化け対策! 「美乳」ではなく「†(ダガー)」を使う | 亜細亜ノ蛾]], [TIME[2025-11-23T07:41:34.000Z]] <https://asiamoth.com/201110222342/>


*** 判定器が必要な場面


[FIG(middle list)[ [26] [[文字コードの判定]]の[[応用]]

- [[encoding sniffing algorithm]]
- [[テキストファイル]]の表示
-- [[テキストエディター]]の「開く」処理
- [[ZIPファイル]]の[[ファイル名]]
- [[RFC 822メッセージ]] ([[MIME]] 以前) 
- [[query parameter]] や[[フォームデータ]]の[[復号]]
- [CODE[mailto:]]
- [[CSV]]
- [[IRC]]
- [[SRT]]
- [[ID3]]
- [[T[SUB[E]]X]]
- [[QRコード]]

]FIG]

-[254] 
[CITE[Googleの検索結果サマリーが半角カタカナだらけ]], [TIME[2020-08-20T23:48:51.000Z]], [TIME[2025-11-23T05:54:04.480Z]] <https://www.shtml.jp/mojibake/google_hankaku.html>
--[255] [CITE[Googleの検索サマリーが「\」やフランス語のアクセント記号だらけ]], [TIME[2020-08-20T23:48:52.000Z]], [TIME[2025-11-23T05:55:00.048Z]] <https://www.shtml.jp/mojibake/google_yen.html>
-- [256] 
[CITE[「痴」「稚」が一杯。英語サイトを作ったら文字化け]], [TIME[2020-08-20T23:48:46.000Z]], [TIME[2025-11-23T05:55:24.455Z]] <https://www.shtml.jp/mojibake/english.html>


[240] 
[CITE@en[1551276 - (chardetng) Autodetect legacy encoding on unlabeled pages]], [TIME[2025-11-23T02:33:03.000Z]] <https://bugzilla.mozilla.org/show_bug.cgi?id=1551276>

[241] 
[CITE@en[Security: ASCII can be autodetected as ISO-2022-JP '''['''40089450''']''' - Chromium]], [TIME[2025-11-23T03:27:53.000Z]] <https://issues.chromium.org/issues/40089450>

[242] >>241 [CITE[Firefox]] の開発者が [[ISO-2022-JP]] を自動判定するのは[[セキュリティー]]上の問題だと主張し、
[CITE[Chrome]] に判定から除外させた回。ところが [CITE[Firefox]] 
は今でも [[ISO-2022-JP]] と判定している。 [TIME[2025-11-23T03:29:38.400Z]]


** 決定に使う入力バイト列の長さと範囲

[SEE[ [[資源ヘッダー]], [[sniffing]], [[encoding sniffing algorithm]] ]]


* 出所とロケール情報による推測

[138] 
判定したい[[バイト列]]の出所 (例えば取得に使った [[URL]] の [[TLD]])
や関係する[[ロケール]]系の情報が[[文字コードの決定]]に使われることがあります。

[86] 
利用し得る情報の例:

- [158] [[バイト列]]の取得に使った情報
-- [139] 取得を始めるために使った [[URL]]
-- [161] [[リンク元]]の [[URL]]
-- [159] [[リンク元]]の[[言語情報]]
-- [160] [[リンク元]]の[[文字コード]]情報 ([[環境符号化]]ほど信用できないもの)
- [146] [[バイト列]]に付随するメタ情報
-- [142] 実際の取得に使った [[URL]] (c.f. [[リダイレクト]])
-- [184] 実際の取得に使った[[ファイル名]]
-- [183] 実際の取得に使った [[Internet Archive]] の [[URL]]
に含まれる原 [[URL]]
-- [165] [CODE[From:]] の[[メールアドレス]]
-- [166] [CODE[Newsgroups:]] の[[ニュースグループ]]
-- [167] [[IRCサーバー]]の[[ドメイン名]]
-- [168] [[IRC]] の[[チャンネル]]
-- [140] [CODE[Content-Location:]]
-- [147] [CODE[Content-Language:]]
-- [192] [CODE[Content-Disposition:]] の [CODE[filename]]
-- [193] [[書庫ファイル]]の格納[[ファイル]]の[[ファイル名]]
-- [151] [[書庫ファイル]]の格納[[ファイル]]の [[OS]] 情報
-- [152] [[書庫ファイル]]の格納[[ファイル]]の作成[[アプリケーション]]情報
-- [162] 兄弟[[バイト列]]の情報
--- [163] 同じ[[書庫ファイル]]の他の[[ファイル]]の[[ファイル名]]とその[[文字コード]]
--- [164] [[RFC 822メッセージ]]の[[ヘッダー]]と[[本体]]の[[文字コード]]
-- [185] [[バイト列]]が[[添付ファイル]]であるときそれが添付された元[[メッセージ]]や元[[メッセージ]]の主たる[[部分]]の情報
--- [186] [[実体]]の[[文字コード]]
--- [190] [[実体]]の [CODE[Content-Language:]]
--- [191] [[実体]]の[[文書要素]]の[[要素の言語]]
- [153] 利用環境に関する情報
-- [154] [[Webブラウザー]]の[[言語]]設定
-- [155] [[プラットフォーム]]の[[ロケール]]設定
--- [156] [[POSIXロケール]]
--- [157] [[ANSIコードページ]], [[OEMコードページ]]

[141] [[URL]] や[[ファイル名]]や[[ドメイン名]]から利用できる情報の例:

- [143] [CODE[file:]] かどうか
- [145] [[TLD]]
- [148] [[URL]] の[[先頭一致]]
- [149] [[言語]]を表す[[拡張子]]
- [150] [[文字コード]]を表す[[拡張子]]

[169] 
利用方法:

- [170] [[文字コード指定メニュー]]の優先表示選択肢の絞り込み
- [171] [[頻度解析等の手法]]の候補の絞り込みや重みの割当
- [172] 他のどの方法でも決定できないときの既定値の選択


-*-*-

[236] 
[CITE@en[845791 - Gather telemetry about the necessity of the Russian and Ukrainian encoding detectors]], [TIME[2025-11-23T02:17:27.000Z]] <https://bugzilla.mozilla.org/show_bug.cgi?id=845791>


[239] >>236 [CITE[Firefox]] が[[キリル文字の文字コード]]の判定を廃止する[[非互換変更]]を企て失敗した回


** TLD の利用

[204] 
[[頻度解析等の手法]]は[[バイト列]]だけでは似た構造の[[文字コードの判定]]に失敗することが少なくないので、
他の情報を補助的に使うことが試みられています。 [[TLD]] 
は特に有力な情報源と考えられています。

[205] 
[[ccTLD]] は、一部の国際的に商業化されたものを除けば、
ほぼ当該[[地域]]で使われています。従って当該[[地域]]の一般的な[[文字コード]]が使われている可能性が、
他の[[地域]]の[[文字コード]]よりずっと高いと考えられます。

[SEE[ [[TLDによる文字コード判定の補助]] ]]

[215] 
ただ、この情報は飽くまでも補助に過ぎません。

- [216] [[IPアドレス]]によるアクセスでは [[TLD]] を使えません。
-- [217] [[逆引き]]や [[IPアドレス]]の割当国データベースに基づく推定も可能ではありますが、
実行コストの高い演算なので、判定ヒントを得るためにしては費用対効果に見合うか疑問です。
- [218] [[gTLD]] では [[ccTLD]] による推定を使えません。
--[219] 昔も今も [[ccTLD]] 以外の [[TLD]] は全世界的によく使われています。
- [220] [[ccTLD]] でも国外で多く使われている事例がいくつもあります。
-- [222] 例えば [CODE[.tv]] は国外のテレビ業界で使われがちです。
-- [221] [[ccTLD]] を使う手法はこうした用途が多い [[ccTLD]] を除外しています。
[SEE[ [[TLDによる文字コード判定の補助]] ]]
--- [223] しかし完璧ではなく、国外利用が多くても除外されていないことがあります。
--- [224] 国内利用についても [[ccTLD]] に基づくヒントを供給できなくなる弊害があります。
- [225] [[正書法]]改革や表記法の対立、内戦などを抱えている国では、
[[ccTLD]] のヒントが機能しにくいことがあります。


[226] 
[[TLD]] は若干の傾斜を与えたり、最終的に判断がつかなかったときの既定値を決めたりするのに使うのがいいのでしょう。
[[TLD]] に基づき候補を絞り込んでそれ以外を除外したりするのは避けておくのが無難です。

[227] 
逆に言うなら、

- [228] [[TLD]] からのヒントがなくてもそれなりに高精度で判別できるようにすること、
- [229] [[文字コード指定メニュー]]のような[[利用者]]が上書きする手段を用意すること、

の'''両方'''が必須です。




- [243] 
[CITE@en[977540 - Don't apply detectors to foreign domains]], [TIME[2025-11-23T03:42:45.000Z]] <https://bugzilla.mozilla.org/show_bug.cgi?id=977540>
-[237] 
[CITE@en[1543077 - Japanese auto-detect no longer work on some generic (i.e. language neutral) TLDs]], [TIME[2025-11-23T02:20:01.000Z]] <https://bugzilla.mozilla.org/show_bug.cgi?id=1543077>

[238] >>237 [CITE[Firefox]] が
[[gTLD]] での[[文字コードの判定]]を廃止する[[非互換変更]]を企て失敗した回

-*-*-

[187] 
[[HTTP]] [CODE[charset]] や [[HTML]] [CODE[<meta charset>]] などで指定された[[文字符号化]]は、
[CODE[.mn]] の場合、次のように置換するべきです。

- [[IBM866]] → [[x-MNS4329]]
- [[windows-1251]] → [[x-MNS4330]]
- [[x-mac-cyrillic]] → [[x-MNS4331]]

** ロケールの利用

[87] [[プラットフォーム]]の[[ロケール]]や[[言語]]の情報が[[文字コードの判定]]のヒントに使われることがあります。

[195] [CITE[ced]] は[[利用者インターフェイス]]の[[言語情報]]があればヒントとして使うことがあります。
[SRC[>>43]]

[38] [[subtitle]] の[[文字コードの判定]]に[[言語情報]]が使われることがあります。
[SEE[ [[ロケール等による文字コード判定の補助]] ]]

[39] [[ZIPファイル]]の[[文字コードの判定]]の補助または既定値の決定に
[[POSIXロケール]]が使われることがあります。
[SEE[ [[ZIPファイルの文字コード]] ]]

[REFS[

- [43] 
[CITE@en[[[compact_enc_det]]/compact_enc_det/compact_enc_det.cc at master · google/compact_enc_det · GitHub]], [TIME[2025-05-19T15:36:17.000Z]] <https://github.com/google/compact_enc_det/blob/master/compact_enc_det/compact_enc_det.cc#L2059>

]REFS]

-*-*-

[178] 
[[HTML]] や[[テキストファイル]]の [[navigate]] では、
他の方法で決められないときの[[既定値]]が[[ロケール]]依存となっています。
[SRC[>>177]]

[179] 
より正確に言えば、[[実装定義]]または[[利用者]]指定の既定の文字符号化とすると定められています。
[SRC[>>177]]
現実的には[[利用者の言語]]から決めることになります。

[180] 
制御された環境や文書の[[符号化]]を予め決められる環境では、 [[UTF-8]]
を[[既定値]]とするのが[RUBYB[よい][suggested]]とされます。
例えば新しい[[ネットワーク]]の専用の[[利用者エージェント]]ではそうできると述べられています。
[SRC[>>177]]

;; [181] 具体的にそのような事例があるのかは不明です。
[[仕様書]]としては可能性を狭めないために「新しいネットワーク」
のようなものを想定しているのでしょうが、
現実的にそうしたものが大々的に導入される機会があるかは不透明です。
(例えば [[HTTPS]] や [[HTTP/2]] への移行でも、[[サーバー]]と[[内容]]は従来のままなので、
切り替えの機会とはできなかったわけで。)
特定の[[イントラネット]]や新しい種類の端末の専用ネットワークでも、
わざわざ既定値を変えるための設定や実装の変更よりは
[[HTTP]] [CODE[charset]] の指定を徹底させる方向性の方が楽そうで。


[182] 
それ以外の環境に対しては、
[[利用者]]の[[ロケール]]が[[利用者]]がよく見る[[Webページ]]の[[自然言語]]や[[符号化]]と相関があると考えられるため、
[[ロケール]]に[RUBYB[典型的には依存][typically dependent]]して既定値が定まるとされます。
[SRC[>>177]]
[SEE[ [[利用者の言語]], [[ロケール依存の既定の文字コード]] ]]


;; [194] [[UTF-8]] は[[頻度解析等の手法]]で高い確率で判定可能です。
であるなら [[UTF-8]] を既定値にするよりも、
既定値は [[Web]] 初期の[[文字コードの指定]]の慣習が無かった時代の
[[Webサイト]]をより良く救済できる可能性が高い値を選ぶのが良いと考えられます。



[REFS[

- [177] 
[CITE@en-US-x-hixie[HTML Standard]], [TIME[2025-11-04T10:59:41.000Z]], [TIME[2025-11-09T05:55:50.836Z]] <https://html.spec.whatwg.org/#determining-the-character-encoding>

]REFS]


[235] [CITE@en[Encoding detector causing compat issues '''['''41301730''']''' - Chromium]], [TIME[2025-11-23T02:10:48.000Z]] <https://issues.chromium.org/issues/41301730>

>I'm not sure when exactly Chromium diverged from WebKit, but the status prior to M55 (for several years) is that Chromium, by default, did no sniffing whatsoever and just used a system locale default.  Secondly, if the user ever clicked "Autodetect" in the encoding menu, this acted as a permanent setting, and in that case ICU autodetector would run on 100% of page loads, overriding all headers, and supporting the entire set of ICU encodings.
>
Starting at M55, we removed all menus and all influence of system locale, and started to run CED autodetector by default but only affecting pages without headers.
>
To my knowledge, Chromium has never shipped a Japanese-specific sniffing configuration.





* 符号構造や出現頻度などによる総合的な推測

[2] 
任意の[[テキストデータ]]の[[文字コード]]の判定には、
[[文字コード]]の[[バイト]]の[[範囲]]や、
出現[[文字]]の頻度・確率の情報が使われています。

[230] 
[[平成時代]]初期 ([[西暦1990年代]]) には [[Web]] 等で[[文字コード]]情報のない[[テキストファイル]]が[[国]]や[[言語]]の境を超えて多く流通するようになり、
[[文字コードの判定]]の手法の研究と実装が各所で行われました。

[5] 
特に[[日本]]と[[キリル文字]]圏では、
複数の[[文字コード]]が同程度に広く使われていたために[[自動判定]]が重宝されていました。


** 符号構造に基づく判別

[231] 
[[文字コードの判定]]の基礎的な技法の1つが[[符号構造]]を利用するものです。


[58] 
[[平成時代]]中頃までの古典的な方法では、
[[文字符号化]]によって[[符号]]の構造が異なることを利用し、
ある[[文字コード]]体系で出現する[[符号]]かそうでないかという構造的知識を主に使っていました。
しかしこの方法単独では[[符号]]構造が重複する領域で互いの区別が付きづらく、
あまり精度が上げられませんでした。
ただ、実装が容易ではあるので、幅広く用いられましたし、現在でも使われることは珍しくありません。

[EG[

[59] 例えば[[シフトJIS]]と[[日本語EUC]]は第1バイトに使われる[[バイト]]、
第2バイトに使われる[[バイト]]の範囲がそれぞれ違っていますので、
その範囲に収まるかによってどちらか判断できることが多いです。
しかし完全に重なる部分もあるため、そのような[[符号]]ばかりだと正しく判定できません。

[60] 
また、[[半角カタカナ]]を利用すると両者の重なる領域が著しく増えるため、
誤判定が多くなり、頻繁に[[半角カタカナ]]の[[文字化け]]を目にすることになります。
これは[[半角カタカナ]]が嫌われる大きな要因の1つにもなっていました。

]EG]


*** UTF-8

[129]
[[UTF-8]] はかなり確実に判定できることが知られています。
[SEE[ [[頻度解析等の手法]] ]]

[288] 
[[UTF-8]] の2バイト以上の[[バイト列]]が含まれ、それ以外に[[非ASCII文字]]が含まれないなら、
[[UTF-8]] と判定してほぼ間違いありません。

[285] 
ただし[[8ビット符号]]の領域を使っている以上誤判定の可能性が皆無ではありません。

[287] 
[CITE[ced]] は4種類の2バイト列を [[UTF-8]] ではなく [[windows-1252]]
に荷重する例外条件を持っています。 [SRC[>>286]]

[REFS[

- [286] 
[CITE@en[[[compact_enc_det]]/compact_enc_det/compact_enc_det.cc at master · google/compact_enc_det · GitHub]], [TIME[2025-11-24T08:13:14.000Z]] <https://github.com/google/compact_enc_det/blob/master/compact_enc_det/compact_enc_det.cc#L83>

]REFS]

** 特に考慮するべき事項

*** ASCII文字の扱い

[305] 
[[EBCDIC]] などを除くと、ほとんどの[[文字コード]]体系は [[ASCII文字]]を共通に持っています。
そのため
[[ASCII文字]]は[[文字コードの判定]]に大きくは寄与しません。

[306] 
また多くの[[マーク付け言語]]や[[プログラミング言語]]は[[英語]]の[[語彙]]を大量に含んでいます。

[307] 
[[HTML文書]]の場合[[要素名]]、[[属性名]]、 [[JavaScript]] コード、
[[CSSスタイルシート]]などの形で多くの[[英語]]の [[ASCII文字]]表記を含みます。
更にいえば、初期の [[Webサイト]]はどの地域でも[[英語]]が多いです。
本文が[[英語]]でないとしても、話題が[[インターネット]]や[[計算機]]の技術系のページでは[[英語]]や[[英語]]由来の
[[ASCII文字]]の語が極めて頻出します。

[315] 
判定に使う文字の出現頻度の情報は、想定される[[自然言語]]の文章から計算されています。
[[英語]]の濃度が極度に大きいと、本来の[[自然言語]]の出現頻度から離れていき、
判定が狂う要因となってしまいます。


[308] 
こうした事情があるので入力の [[ASCII文字]]をどう取り扱うかは設計上無視できない問題となります。

[309] 
[[ASCII文字]]を無視すれば、こうした「ノイズ」も一気に除去することができ、
[[非ASCII文字]]の[[文字コードの判定]]に注力し、関係ない部分の処理の負担を軽減できます。

[310] 
一方で欧州言語など[[ASCII文字]]が[[言語]]の表記の主体となる場合、
[[非ASCII文字]]の割合が少ないので、
すべての[[ASCII文字]]を捨ててしまうと[[言語判定]]の重要な情報まで捨ててしまうことになります。

[311] 
中間解として、 [[非ASCII文字]]の周囲の [[ASCII文字]]を判定に活用するのが現実的です。
[[ASCII文字]]と[[非ASCII文字]]にまたがる [[n-gram]] の出現頻度は、
とりわけ欧州言語の判定に重要です。

[312] 
[CITE[UnivCharDet]] は[[非ASCII文字]]を含む単語を[[8ビット符号]]の判定に利用しています。

[313] 
[CITE[chardetng]] はそれより攻めていて [[ASCII文字]]同士の[[連接]]は判定に使わないようです。

[314] ただしこうした戦略の違いがどれだけ判定性能や動作速度や消費メモリー量に影響を及ぼすのか、
定量的な比較はあまり行われていないようで不明瞭です。

[316] 
欧州語で、しかも[[ファイル名]]のような短い文字列が入力のとき、
[[ASCII文字]]だけの部分でも[[言語判定]]のヒントに使えれば、
数少ない[[非ASCII文字]]やその前後だけでは情報が不足する[[言語判定]]の補強材料になります。
しかし両者が関係ない単語の場合もあって、そのときは誤判定のリスクが増大します。


*** 欧州ラテン文字系文字コードの区別

[200] 
[[Windows-1252]] (含 [[ISO-8859-1]]) と [[ISO-8859-2]] と [[Windows-1250]]
は区別の難易度が高いことが知られています。

[201] 
そもそも[[ラテン文字]]系[[言語]]は文章の多くが [[ASCII文字]]で、
[[言語]]次第で少々の[[非ASCII文字]]が混ざるという構造です。
[[非ASCII文字]]主体の他[[文字]]の[[言語]]よりも判別が難しいです。

[202] 
[[Windows-1252]] と [[Windows-1250]] は似た構造ですが、
収録される[[文字]]の種類は一部で著しく異なっています。
文章に少々混じる[[非ASCII文字]]のうちの更に一部の頻出文字が共通で、
残りが全く異なるので、[[バイト]]の並びとして見たとき、
どちらかにわかに判断しがたいことが多いです。

[203] 
[[Windows-1250]] と [[ISO-8859-2]] はだいたい同じで少し違います。
どちらも[[中欧]]でよく使われていた[[文字コード]]で、
同じような[[言語]]で同じように使われていて、
わずかな違いがどちらなのか判定するのが難しいです。


[211] 
[[Mozilla]] の [CITE[UnivCharDet]] は [[ISO-8859-2]] に対応しているものの、
[[ISO-8859-1]] が誤判定されてしまうとして無効化されています。
[CITE[UnivCharDet]] の派生の中にはこれを改善して有効化しているものもありますが、
それらも完璧に判定できるわけではありません。


[212] 
[CITE[ced]] は [CODE[.hu]] [[ドメイン]]のみ [[trigram]] を有効にするなど、
特別な処理で判定を強化しています。それでも [CITE[Chrome]]
はたまに判定を誤ります。
[SEE[ [[ロケール等による文字コード判定の補助]] ]]

[213] 
[CITE[chardetng]] も [[TLD]] による傾斜など特別な処理で判定を強化しています。
それでも [CITE[Firefox]] はしばしば判定を誤ります。
[SEE[ [[ロケール等による文字コード判定の補助]] ]]

[214] 
完璧な判定は困難ですから、どの手法を採るにせよ、最終的に[[文字コード指定メニュー]]など[[利用者]]が選択を覆せる手段が必須です。

[EG[

[281] >>280 は[[ハンガリー語]]で書かれた [[HTML]] です。
各種の判定器は [[ISO-8859-2]], [[windows-1250]], [[windows-1252]]
と判断が分かれています。
[CITE[Firefox]] は [[windows-1252]] と考えます。
[CITE[Chrome]] は [[windows-1252]] と考えますが、
[CITE[ced]] にバイト列を与えると [[windows-1250]] と回答します。
[TIME[2025-11-24T06:15:55.100Z]]

[282] 実際には3符号化の共通文字が[[非ASCII文字]]でも多いのですが、
[[ハンガリー語]]であることと[[ハンガリー語]]の[[文字]]の使い方から
[[ISO-8859-2]] / [[windows-1250]] と考えるのが妥当です。
>>280 の範囲では [[ISO-8859-2]] と [[windows-1250]] は同等です。

;; [283] [CITE[Firefox]] と [CITE[Chrome]] は[[文字コード指定メニュー]]がないので、
[[文字化け]]したまま修復できません。一種の不具合といえます。

[REFS[

- [280] 
[CITE[Bemutatkozik a WILD WEST GYÕR]], [TIME[2025-11-24T06:08:55.000Z]] <https://www.members.tripod.com/wildwestgyor/bemut.htm>

]REFS]

]EG]

*** 大文字と小文字

[294] 
出現頻度による手法の多くは[[大文字と小文字]]を同一視して[[文字]]や [[n-gram]]
の頻度を見ています。ほとんどはこの手法でうまく判定できます。

[292] 
[CITE[chardetng]] は[[大文字と小文字]]の使い分けが自然なものを加点し不自然なものを減点しています。
[SRC[>>291]]

[293] 
[[ギリシャ文字]]ですべて[[大文字]]にした語は、[[キリル文字]]をすべて[[小文字]]にした
[[KOI-8]] や[[ヘブライ文字]]と混同しやすいとされ、
[CITE[ced]] や [CITE[chardetng]] が特殊処理を入れています。
[SRC[>>291, >>284]]

[300] 
なお [CITE[chardetng]] は[[ラテン文字]]とそれ以外の隣接で減点しています。
[SRC[>>299]]

[REFS[

- [291] 
[CITE@en[chardetng/src/lib.rs at main · hsivonen/chardetng · GitHub]], [TIME[2025-11-24T08:24:04.000Z]] <https://github.com/hsivonen/chardetng/blob/main/src/lib.rs#L154>


[FIG(quote)[
[FIGCAPTION[
[284] [CITE@en[[[compact_enc_det]]/compact_enc_det/compact_enc_det.cc at master · google/compact_enc_det · GitHub]], [TIME[2025-11-24T07:39:23.000Z]] <https://github.com/google/compact_enc_det/blob/master/compact_enc_det/compact_enc_det.cc#L1770>
]FIGCAPTION]

>        // Greek all-caps is confusable with KOI8x all-lower and Hebrew.
]FIG]

- [299] 
[CITE@en[Countermeasures for various misdetections. · hsivonen/[[chardetng]]@0973b4b · GitHub]], [TIME[2025-11-24T08:50:10.000Z]] <https://github.com/hsivonen/chardetng/commit/0973b4b67da81b9be2f643d0da70536d616aec06>


]REFS]

*** 語長

[297] 
[CITE[chardetng]] は[[語長]]が23[[超]]なら[[タイ文字]]以外を減点しています。
[SRC[>>296]]

[298] [CITE[chardetng]] は [[EUC-KR]] の[[ハングル]]の[[語長]]が5[[超]]なら減点
([CODE[EUC_KR_LONG_WORD_PENALTY]]) しています。 [SRC[>>296]]

[301] 
他の実装にはあまり見られないので有効性は不明瞭です。

[302] 
[[johab]] は他の符号と誤認の排除に[[ハングル]]の[[語長]]の平均 (大雑把に [N[3]]
字くらい) との乖離の検知が有効なようです。


[REFS[

- [296] 
[CITE@en[[[chardetng]]/src/lib.rs at main · hsivonen/chardetng · GitHub]], [TIME[2025-11-24T08:46:35.000Z]] <https://github.com/hsivonen/chardetng/blob/main/src/lib.rs#L755>

]REFS]

*** 8ビット符号の特徴的な符号列


[265] 
かなり多くの[[Webページ]]が[CH[©]]を使った[[著作権表記]]を持っています。
[CH[©]]の前は空白か[[タグ]]で、
[CH[©]]の後は空白か[[タグ]]か、[[年号]]か[[著作権者]]が来ます。

[266] 
[[windows-1252]] やいくつかの[[文字コード]]体系で [N[0xA9]] に [CH[©]]
があります。 >>265 のような[[バイト列]]のパターンを発見できれば、
そうした[[文字コード]]体系である可能性が相当高くなります。

[269] 
[N[0xA9]] は [[Shift_JIS]] で[[半角カナ]]の [CH[ゥ]] に当たります。
幸い [CH[ゥ]] は直前に[[カタカナ]]が来ることがほとんどで、 >>265 
のようなパターンで出現することはまずありません。
[[Shift_JIS]] でないことを示す徴証として使うのが良いと考えられます。

[318] 
他の[[多バイト符号]]では第1バイトに使われることがありますし、
[[Shift_JIS]] でも第2バイトには使われることがあります。
前後が[[空白]], という条件が重要になります。
多くの[[多バイト符号]]だと第2バイトに [CH[<]] や [CH[>]] が来ることもないので、
[[タグ]]も[[空白]]と同等に扱えます。
それ以外だと徴証としては少し弱くなります。

[317] 
[[windows-1252]] やいくつかの[[文字コード]]体系で [N[0xAE]] に 
[CH[®]] があります。
[N[0x99]]
に
[CH[™]]
があります。これらは語末に出現します。

[319] 
[[英数字]]の後に[[半角カナ]] [N[0xAE]] 
が1つだけ出現して[[空白]]が来ることはほとんどないので、
[[Shift_JIS]] ではない可能性が高いと判断できます。

[320] 
しかし [N[0xAE]] や [N[0x99]] は[[多バイト符号]]の第2バイトになることがあるので、
[[非ASCII文字]]の後に [N[0xAE]] や [N[0x99]]
が来るケースでは注意が必要です。

[EG[

[321] 
[CITE[ced]] は 「NESTLÉ®」 を [[UTF-8]] と誤認される [[windows-1252]] 
の実例として挙げています。この例のように [[Shift_JIS]] 等の他に
[[UTF-8]] としても正当なバイト列になり得ることがあるので注意が必要です。


]EG]

[267] 
他に[[非ASCII文字]]がなくても[CH[©]]や[CH[®]]だけが[[非ASCII文字]]として含まれることが、
[[欧米]]や[[中央アジア]]をはじめ、世界的によく見られます。
そうした場合にこれを [[Windows-1252]] と判断することが重要になってきます。


;; [268] [[言語]]モデルによる判定は [[letter]] だけを使いがちで、
[CH[©]]
のような記号が除外されていて判定に使われず、[[文字化け]]してしまうことがあります。


[270] [CITE[chardetng]] などがこうしたものを
[[windows-1252]] と判定する条件を組み込んでいます。
[SRC[>>289]]



;; [290] ただし [CITE[chardetng]] はそれでも 
[CH[©]]
を
[[ISO-8859-2]] と誤認しがちです。
[[ISO-8859-2]] との区別についてはわざわざ注釈で言及があります [SRC[>>289]]
ので、意識して設計されているはずですが、それでも取り扱いが難しいということなのでしょう。

[EG[

[330] 
>>329
は
「Copyright ©1997,」
の [CH[©]] 1文字 ([N[0xA9]])
だけが[[非ASCII文字]]です。
[CITE[ced]] は正しく判定しますが、
[CITE[chardetng]] は [[ISO-8859-2]] と誤認します。


[REFS[

- [329] 
[CITE[Open Society-Georgia Foundation ([[OSGF]])]], [TIME[2025-11-30T03:20:04.000Z]] <https://web.archive.org/web/19981202173516id_/http://www.osgf.ge/>

]REFS]

]EG]

[REFS[

- [289] 
[CITE@en[[[chardetng]]/src/lib.rs at main · hsivonen/chardetng · GitHub]], [TIME[2025-11-24T08:19:16.000Z]] <https://github.com/hsivonen/chardetng/blob/main/src/lib.rs#L58>
- [295] 
[CITE@en[[[chardetng]]/src/lib.rs at main · hsivonen/chardetng · GitHub]], [TIME[2025-11-24T08:34:54.000Z]] <https://github.com/hsivonen/chardetng/blob/main/src/lib.rs#L450>


]REFS]


*** 価格

[322] 
[[windows-1252]] などいくつかの[[コードページ]]の [N[0x80]] は [CH[€]]
です。[[通貨記号]]の後に[[ASCII数字]]が続くなら、
[[価格]]の表記と考えられ、
その[[文字コード]]体系であることのヒントとして使えます。

;; [323] [[言語]]モデルによる判定は [[letter]] だけを使いがちで、
[[通貨記号]]や[[数字]]が除外されていて判定に使われず、[[文字化け]]してしまうことがあります。

[324] 
[N[0x80]] は主要な[[多バイト符号]]の第1バイトには使われないので、
空白の後などで重要なヒントとして使えます。
しかし第2バイトに使われることはあるので、注意が必要です。

[325] 
[[windows-1252]] の [N[0xA4]] など他の[[通貨記号]]は[[多バイト符号]]の第1バイトや第2バイトや、
[[Shift_JIS]] の[[半角カナ]]に使われることがあるので、注意が必要です。

*** 罫線素片

[326] 
多くの [[OEMコードページ]]は[[罫線素片]]等の [[CUI]] 
描画のための部品[[文字]]を多く持っています。
これらは通常の文章には出てこないことが多いですが、図表などで使われることもあります。

[327] 
こうした文字が1つだけ孤立して出現することはまずないので、
負のヒントとして使うことができます。

[328] 
ただし縦線はそこだけ見ると前後に別の文字が来る、孤立した文字に見えますから、
単に前後が[[罫線素片]]でないというだけでは足りず、少し工夫が必要です。
前後の行との結合を検査すれば確実ですが、
そこまでせずとも、
他に横線 ([[罫線素片]]の連続) があるなら同様に[[罫線素片]]とみなしてよく、
他に横線がどこにもないなら負のヒントとみなすのがいいかもしれません。


*** EUC-JP の特徴的な符号列

[271] 
[[EUC-JP]] の [[CS3]] は [[JIS X 0212]] ですが、
先頭付近のいくつかの[[区]]に[[ダイアクリティカルマーク]]付き[[アルファベット]]等が配置されています。
こうした[[文字]]の使われ方を想像すると、
[[欧州]]の[[言語]]の[[単語]]を表す通常の[[アルファベット]]の列の中に孤立して1つだけ混じることが多そうです。
2つ以上続くこともあるでしょうが、1つだけのことが多そうです。


[272] 
例えば [[ASCII文字]]に囲まれた [N[0x8F]] ([CN[SS3]]) と [[GR]] 
のバイト2つで構成される3バイトの列は、 >>271 の文字である可能性が高そうです。
各種の[[8ビット符号]]でもこのようなバイト列が出現することは考えられますが、
この特徴的な並びが意味のある[[言語]]の語を構成することは余り多くはなさそうです。
そこでこうした並びを数えて、
[[EUC-JP]]
と判定する有力な根拠として使うことができます。

[273] 
実際のところ [[EUC-JP]] でこれらの[[文字]]を使った (しかし一般の[[日本語文字]]はあまり使わない)
文章はそこまで多くないと思われます。
[[文字コードの判定]]のためのライブラリーのテストデータに含まれていることもあるのですが、
テスト用に変換して人工的に作った例文と思われます。

[274] 
よってあまり優先度は高くありませんが、簡単に対応できるならしておいても良いかなという感じでしょうか。

*** gb18030 の特徴的な符号列

[275] 
[[gb18030]] の4バイト符号は、第1バイトと第3バイトに[[右]]の[[バイト]]が使われ、
第2バイトと第3バイトは [[ASCII数字]]の[[バイト]]が使われるというかなり独特の構造をしています。
主要な[[多バイト符号]]で[[ASCII数字]]を第2バイトに使うのは [[gb18030]]
だけです。
各種の[[8ビット符号]]でこうした並びが出現することもあるでしょうが、
特殊な用例に限られるのではないかと思われます。
そこでこうした並びを数えて、
[[gb18030]]
と判定する有力な根拠として使うことができます。



*** エスケープとの混合

[188] 
[[エスケープ]] ([[HTML]] の[[文字参照]]など) とそうでない通常の[[文字]]が混合されている場合、
純粋な[[文字列]]とは違った[[文字]]分布になってしまう場合があります。

[189] 
単純に [[ASCII文字]]だから、[[マーク付け言語]]の構文要素だから、
といった理由で[[エスケープ]]を除去すると、通常の[[文字]]の前後関係が[[言語]]の一般的なパターンと外れてしまい、
判定に失敗することがあります。

[196] 
[[エスケープ]]とそうでない[[文字]]が混在するのは、特に理由が無いこともありますが、
敢えて混在させていることもあります。[[文字化け]]しやすいとか、
その[[文字コード]]に存在しないとかです。得てしてそれらは[[文字コードの判定]]の際どい条件に関わってくる要素になりがちです。

*** 書字方向

[303] 
[[ISO-8859-8]] など[[視覚順]]と[[論理順]]の区別が必要となる場合があります。

[304] 
[[ヘブライ文字]]は[[語末形]]と[[語中形]]が異なるものが5字10種あります。
[[ISO/IEC 8859-8]] や [[Windows-1255]] はそれらに別の[[ビット組合せ]]を充てています。
[CITE[UnivCharDet]] は[[ヘブライ文字]]列の先頭や末尾の[[字形]]の個数や前後から見た [[bigram]]
の評価によってどちらか判定しています。


** 実装

*** 出現頻度等による実装

[176] 出現頻度等による実装:

[REFS[

- [35] [CITE[[[UniversalCharDet]]]] の系譜
- [36] 
[CITE@en[[[GitHub]] - chomechome/charamel: 🌏 Truly Universal Encoding Detection in Python 🌎]], [TIME[2025-05-19T12:46:16.000Z]] <https://github.com/chomechome/charamel>
- [37] 
[CITE@en[GitHub - jawah/charset_normalizer: Truly universal encoding detector in pure Python]], [TIME[2025-05-19T12:51:46.000Z]] <https://github.com/jawah/charset_normalizer>
- [40] 
[CITE@en-US[Charset Detection | ICU Documentation]], [TIME[2025-04-15T18:50:10.000Z]], [TIME[2025-05-19T13:56:04.594Z]] <https://unicode-org.github.io/icu/userguide/conversion/detection.html>
- [41] 
[CITE@en[GitHub - hsivonen/shift_or_euc: Detects among the Japanese legacy encodings]], [TIME[2025-05-19T13:59:56.000Z]] <https://github.com/hsivonen/shift_or_euc>
-- [62] 
[[日本語]]系[[文字コード]]の判定
-
[91] [CITE@en[google/compact_enc_det: compact_enc_det - Compact Encoding Detection]]
([TIME[2016-07-30 15:05:47 +09:00]])
<https://github.com/google/compact_enc_det>
-- [42] 
[CITE@en[GitHub - google/compact_enc_det: compact_enc_det - Compact Encoding Detection]], [TIME[2025-05-19T14:08:52.000Z]] <https://github.com/google/compact_enc_det/>
-
[45] 
[CITE@en-US[Encode::Guess::Educated - do something - metacpan.org]], [TIME[2025-05-20T15:01:04.000Z]] <https://metacpan.org/pod/Encode::Guess::Educated>
-[81] 
[CITE@en[GitHub - vlm/zip-fix-filename-encoding: Fix cyrillic character encoding of filenames inside zip archives]], [TIME[2025-05-16T10:03:38.000Z]] <https://github.com/vlm/zip-fix-filename-encoding>
-- [144] 
[CITE@en[zip-fix-filename-encoding/src/runzip.c at master · vlm/zip-fix-filename-encoding · GitHub]], [TIME[2025-05-24T14:05:58.000Z]] <https://github.com/vlm/zip-fix-filename-encoding/blob/master/src/runzip.c>
-- [61] 
[[キリル文字]]系[[文字コード]]の判定
- [84] [CITE[Wayback Machine]], [TIME[2025-06-02T11:58:14.000Z]] <https://web.archive.org/web/20250601053528/https://shoshia.tripod.com/pub/webconv.zip>
-- [64] [CODE[GEO-CONV.PL]]
--- [65] [CODE[&analyze]]
---- [66] >>64 は[[ジョージア文字]]の変換器ですが、その改造元は[[キリル文字]]の変換器で、
(おそらく改変されていない)この関数は[[キリル文字]]の[[文字コード]]を出現頻度で推定するものです。
[[注釈]]によると
[[Stefan Mashkevich]]
が[TIME[1998-11-26]]に開発したものです。
KOI8, DOS866, WIN1251, ISO8859-5, MAC に対応しています。
- [67] 
[CITE@en[Universal online Cyrillic decoder - recover your texts]], [[Petko Yotov]], [TIME[2025-06-04T07:48:52.000Z]] <https://2cyr.com/decode/>
- [73] 
[CITE@en-US[Encode::Detect::CJK - A Charset Detector, optimized for EastAsia charset and website content - metacpan.org]], [TIME[2025-06-25T08:14:27.000Z]] <https://metacpan.org/pod/Encode::Detect::CJK>
- [77] 
[[チベット文字の文字コード]]にも[[チベット文字]]の[[符号]]の判定手法についてあり

]REFS]

-*-*-

[3] [[universalchardet]] は、 [[Mozilla]] が [[Webページ]]の表示のために開発したものです。
多くの[[プラットフォーム]]に移植されて使われています。

[4] 次の[[符号化]]に対応しています:

utf-8 utf-16be utf-16le iso-2022-cn big5 x-euc-tw gb18030 hz-gb-2312
iso-2022-jp shift_jis euc-jp iso-2022-kr euc-kr 
[CODE[iso-8859-5]] 
koi8-r
[CODE[windows-1251]]
x-mac-cyrillic ibm866 ibm855 iso-8859-7 
[CODE[tis-620]]
windows-1253
iso-8859-8 windows-1255 windows-1252

;;
[6] 
データだけで未実装:
[CODE[iso-8859-2]]
[CODE[windows-1250]]

-*-*-

[276] 
[DFN[[CITE[compact_enc_det]]]] ([DFN[[CITE[ced]]]])
[SRC[>>91]]
は、
[[Google]] による[[文字コードの判定]]の[[オープンソース]]ライブラリーです。

[277] 
[CITE[Google Chrome]] で採用されています。 [[Google]] 社内の[[検索]]や [CITE[Gmail]]
などでも使われていると言われています。

[278] 
この種のライブラリーの中でも精度は高いです。 [[Google検索]]で使われる
[[Google]] 社内の世界最大規模の [[Webページ]]データベースの解析の成果が反映されていると見られます。
ソースコード中でもいろいろな調整が入っている様子が窺えます。

[279] 
ただ逆に言えば[[オープンソース]]とはいえ [[Google]] 社員以外がこれを改善する改変を行うことは困難で、
そのまま使うか、他のソフトウェアの改善のヒントに使うくらいしかできません。

-*-*-

[82] 
[DFN[[CITE[Charamel]]]]
は [[Python]] 用[[ライブラリー]]です。[[機械学習]]によって [[Python]]
が標準で対応するすべての[[符号化]]に対応したと謳っています。
[SRC[>>36]]

[83] 
実際に判定させてみると、他の判定器と比べて精度は今ひとつのようにも思われます。
その中には類縁の他の[[符号化]]と判断されたものがあり、
使用している[[文字]]次第でどちらとも判定できるので誤判定ではないと言えるものもありますが、
それらを除外しても不一致が多いように感じられます。
[[機械学習]]の方法による不透明なバイナリーデータを判定に用いているため、
改善も困難と思われます。

[85] 
付属の試験データ [SRC[>>31]]
は実際の [[Webページ]]らしきものやその他の[[テキスト]]データが含まれますが、
各[[文字コード]]には機械的に変換したものと見られます。
中には[[非ASCII文字]]が1つも含まれないデータしかない[[符号化]]もあり、
試験データとして精査されたものとは思えません。


@@
[88] 
[CITE@en[Expose apparent_encoding_confidence. by martinblech · Pull Request #1796 · psf/requests · GitHub]], [TIME[2025-10-19T13:06:53.000Z]] <https://github.com/psf/requests/pull/1796>

*** 符号構造のみによる実装


[54] [[符号構造]]のみによる実装:

[REFS[

-
[63] 
[CITE[null]], [TIME[2008-05-10T18:15:22.000Z]], [TIME[2025-05-24T14:10:44.885Z]] <http://openlab.ring.gr.jp/Jcode/Jcode.pm#:~:text=sub%20getcode>
- 
[44] 
[CITE@en-US[Encode::Guess - Guesses encoding from data - metacpan.org]], [TIME[2025-05-20T14:53:40.000Z]] <https://metacpan.org/pod/Encode::Guess>
-
[72] 
[CITE@en-US[Encode::HanDetect - Cross-encoding, cross-variant Chinese decoder - metacpan.org]], [TIME[2025-06-25T08:00:21.000Z]] <https://metacpan.org/pod/Encode::HanDetect>
--[74] [CITE@en-US[HanDetect.pm - metacpan.org]], [TIME[2003-06-27T04:53:28.000Z]], [TIME[2025-06-25T08:15:33.569Z]] <https://metacpan.org/module/Lingua::ZH::HanDetect/source>
-
[53] 
[CITE@ja[文字エンコーディング判定スクリプト - t_komuraの日記]], [TIME[2025-05-21T12:18:53.000Z]] <https://t-komura.hatenadiary.org/entry/20091220/1261305552>
-
[75] 
[CITE@en-US[Encode::Multibyte::Detect - detect multibyte encoding - metacpan.org]], [TIME[2025-06-25T08:18:04.000Z]] <https://metacpan.org/pod/Encode::Multibyte::Detect>


]REFS]

[76] [CITE[LV Homepage (in Japanese)]], [TIME[2025-06-25T14:34:54.000Z]], [TIME[2001-01-19T05:51:42.277Z]] <https://web.archive.org/web/20010119052900/http://www.mt.cs.keio.ac.jp/person/narita/lv/index_ja.html>

>現在の自動選択の方法は簡単なものです. ファイルを先頭から読み込んでいって, 8ビット目が立っている文字があった場合, 『その行』の中で euc-japan で使用される領域のみを使っていれば euc-japan, そうでなければ shift-jis です. つまり,『漢字らしきものを含む最初の一行』で判断しています. 8ビット目が立っている文字が見つからなければ いつまでも自動選択のままの状態が続き, 判断が必要になったときに判断します. shift-jis の片仮名のみを使用した場合や, 運の悪いときは, 誤って euc-japan と認識されます.


[70] 
[CITE@en[Add UTF-7 to replacement encoding list? / Encoding sniffing · Issue #68 · whatwg/encoding]], [TIME[2025-06-17T03:18:44.000Z]] <https://github.com/whatwg/encoding/issues/68>

[71] 
[CITE@en[Encoding: make it clear sniffing for UTF-8 is not acceptable by annevk · Pull Request #14455 · web-platform-tests/wpt · GitHub]], [TIME[2025-06-17T03:20:29.000Z]] <https://github.com/web-platform-tests/wpt/pull/14455>

[199] [CITE@ja[Escape Codec Library: ecl.js]], [TIME[2012-04-16T04:12:06.000Z]], [TIME[2025-11-19T14:41:59.833Z]] <https://www.junoe.jp/downloads/itoh/enc_js.shtml>

[198] [CITE@ja[ISO-2022-JPとSJISとEUCJP(とUTF-8)をざっくり判別するアルゴリズム - うならぼ]], [TIME[2025-11-19T14:38:09.000Z]] <https://unarist.hatenablog.com/entry/2017/02/28/205401>


** テストデータ

[REFS[

- [7] 
[CITE@en[gecko-dev/extensions/universalchardet/tests at master · mozilla/gecko-dev · GitHub]], [TIME[2025-05-17T09:04:18.000Z]] <https://github.com/mozilla/gecko-dev/tree/master/extensions/universalchardet/tests>
-- [8] [[MPL]] 2
- [9] [CITE@en[juniversalchardet/data at master · seratch/juniversalchardet · GitHub]], [TIME[2025-05-18T03:30:23.000Z]] <https://github.com/seratch/juniversalchardet/tree/master/data>
-- [10] [[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+
- [11] 
[CITE@en[juniversalchardet/src/test/resources at main · albfernandez/juniversalchardet · GitHub]], [TIME[2025-05-18T03:36:18.000Z]] <https://github.com/albfernandez/juniversalchardet/tree/main/src/test/resources>
-- [12] [[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+
-- [13] >>9 を含み、更に追加
- [14] 
[CITE@en[ude/src/Tests/Data at master · errepi/ude · GitHub]], [TIME[2025-05-18T03:40:02.000Z]] <https://github.com/errepi/ude/tree/master/src/Tests/Data>
-- [15] 「[[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+」、
「Wikipedia と同じ」、
「The Project Gutenberg と同じ」
が混在
-- [16] >>9 を含み、更に追加
- [17] 
[CITE@ja[test · master · uchardet / uchardet · GitLab]], [TIME[2025-05-18T04:21:47.000Z]] <https://gitlab.freedesktop.org/uchardet/uchardet/-/tree/master/test?ref_type=heads>
-- [18] [[MPL]] 1.1 / [[GPL]] 2+ / [[LGPL]] 2.1+
- [19] 
[CITE@en[rust-chardet/tests/data at master · thuleqaid/rust-chardet · GitHub]], [TIME[2025-05-18T05:01:25.000Z]] <https://github.com/thuleqaid/rust-chardet/tree/master/tests/data>
-- [20] [[LGPL]] 3
- [21] [CITE@en[chardet/tests at main · chardet/chardet · GitHub]], [TIME[2025-05-18T05:10:41.000Z]] <https://github.com/chardet/chardet/tree/main/tests>
-- [22] 不自由なものを含む
---
[28] 
[CITE@en[problematic licensing of /tests? · Issue #231 · chardet/chardet]], [TIME[2025-05-19T06:27:37.000Z]] <https://github.com/chardet/chardet/issues/231>
--- [27] 
[CITE@en[Documentation licensed only to non-commercial and personal use found · Issue #271 · chardet/chardet]], [TIME[2025-05-19T06:24:05.000Z]] <https://github.com/chardet/chardet/issues/271>
- [29] 
[CITE@en[GitHub - Ousret/char-dataset: Public dataset used to challenge Charset-Normalizer]], [TIME[2025-05-19T08:37:52.000Z]] <https://github.com/Ousret/char-dataset>
-- [30] ライセンス不明
- [31] 
[CITE@en[charamel/tests/fixtures at master · chomechome/charamel · GitHub]], [TIME[2025-05-19T11:59:39.000Z]] <https://github.com/chomechome/charamel/tree/master/tests/fixtures>
-- [32] [[Apache 2.0]] となっているが、出所の怪しげなファイルもある
-- [33] [[Git LFS]]
-- [34] 機械的に変換したファイルが多い?
- [46] 
[CITE@en[compact_enc_det/compact_enc_det/compact_enc_det_unittest.cc at master · google/compact_enc_det · GitHub]], [TIME[2025-05-21T06:26:33.000Z]] <https://github.com/google/compact_enc_det/blob/master/compact_enc_det/compact_enc_det_unittest.cc>
-- [47] [[Apache 2.0]]
-- [48] [[C++]] ソースコードに埋め込まれている


]REFS]


* 関連

[55] [[文字コード選択メニュー]]

* メモ


[80] 
[CITE@en[How to reliably guess the encoding between MacRoman, CP1252, Latin1, UTF-8, and ASCII - Stack Overflow]], [TIME[2025-10-18T08:36:44.000Z]] <https://stackoverflow.com/questions/4198804/how-to-reliably-guess-the-encoding-between-macroman-cp1252-latin1-utf-8-and>


- [51] 
[CITE@ja[mb_detect_encoding() は文字コード判定として使用できるか(その1) - t_komuraの日記]], [TIME[2025-05-21T12:15:39.000Z]] <https://t-komura.hatenadiary.org/entry/20090615/1245078430>
-- [50] 
[CITE@ja[mb_detect_encoding() は文字コードの妥当性検証として使用できるか(その2) - t_komuraの日記]], [TIME[2025-05-21T12:15:09.000Z]] <https://t-komura.hatenadiary.org/entry/20090621/1245595484>
-- [52] 
[CITE@ja[mb_detect_encoding() は文字コードの妥当性検証として使用できるか(その3) - t_komuraの日記]], [TIME[2025-05-21T12:17:54.000Z]] <https://t-komura.hatenadiary.org/entry/20090705/1246802467>





[49] [CITE@en['''['''ptexenc''']''' 入力ファイルの文字コードの自動判定 · Issue #142 · texjporg/tex-jp-build]], [TIME[2025-05-21T12:13:06.000Z]] <https://github.com/texjporg/tex-jp-build/issues/142>



- [68] [CITE@en-US[Encode::Detect::Upload - Attempt to guess user's locale encoding from IP, HTTP_ACCEPT_LANGUAGE and HTTP_USER_AGENT - metacpan.org]], [TIME[2025-06-16T10:29:46.000Z]] <https://metacpan.org/pod/Encode::Detect::Upload>

[69] >>68 [CODE[Accept-Language:]], [CODE[User-Agent:]] ([[OS]]), 
アクセス者の [[IPアドレス]]を使って[[文字符号化]]を推定する。
平成25年。



[78] 
対応している[[符号化]]を順番に試してエラーにならなかったものを採用するという実装を「文字コードの判定」だと称しているものがたまにあります。
このような方法は[[符号構造]]がまったく違う[[符号]]の区別になら使えますが、
多くの[[8ビット符号]]の区別が不可能です。

;; [79] [[符号構造]]が限定される場合なら、その限定される特徴で判定したほうが高速かつ確実なことが多いので、
この手法が役に立つことはほぼないといっていいでしょう。




[89] [CITE@en[21990 – When a rare EUC-JP character is present, explicitly (and correctly) labelled EUC-JP document is mistreated as Shift_JIS]], [TIME[2025-11-16T05:48:11.000Z]] <https://bugs.webkit.org/show_bug.cgi?id=21990>

[90] 
[CITE@en[16482 – Hook up ICU's encoding detector and add a boolean param to Settings and WebPreferences]], [TIME[2025-11-16T05:50:12.000Z]] <https://bugs.webkit.org/show_bug.cgi?id=16482>
