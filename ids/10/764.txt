#?SuikaWiki/0.9

[1]
search-result も page-list も切ってみて、一晩無事に過ぎました。やっぱりこいつらが元凶のようです。

search-result は [[Namazu]] 版を作りましょう。 page-list も Namazu 版を・・・というのはなんだか変な気もするなあ。
変だけど page-list と recent-changes-list (それにもちろん search-result) は上限を設けて、それ以上の時は Namazu 版を使え、ということにしておけばいいかな。
([[名無しさん]] [WEAK[2004-03-15 01:35:29 +00:00]])

[2]
[[Search::Namazu]] 使ってみました。わかったこと:
- Search::Namazu::Search の時間経費はかなり安い。むしろ WikiLinking や Message::Markup::XML::Node が遅い。
- (WL や MMXN を含めた) 出力の時間は、結果項目数の一乗だか二乗だか対数だかしらないけど、それに比例する。 (当たり前か。)
- 検索の出力よりも SuikaWiki09 の出力の方が時間がかかることがある(w

出力も一度に一定数しかしないようにしておけば大した速度低下にはならなそうです。

([[名無しさん]] [WEAK[2004-03-15 08:22:08 +00:00]])

[3]
また暴走しちゃいました。。。search-result とかは切ったままなのに。

出力に時間のかかる頁の処理が滞ってるみたいですけど、多分それだけが原因じゃなくてその前後も関係しているのでしょうな。

その前後には糞ロボットが一秒間に数個の絨毯爆撃しているみたいです。 [CODE(HTMLe)[[[link]]]] 要素のリンク先を順番に拾っていってます。。。
([[名無しさん]] [WEAK[2004-03-15 09:26:09 +00:00]])

[4]
で、滞った最初の方では RandomJump みたいな比較的早い要求も終えられないのに対して、強制的に止める直前の read 要求はすぐに終えてるものもあるのです。
つまり down しちゃってるわけじゃないので、十分な時間待てば復帰する、のかなあ。

同時に実行できる wiki.cgi を十個くらいに制限してみればいいのかな。 (Apache にはそんな感じの指令は標準モジュールにはない。。)

([[名無しさん]] [WEAK[2004-03-15 09:31:49 +00:00]])

[5]
で、テストしようとしているときに限って全然 access がないんだな、これが:-<
([[名無しさん]] [WEAK[2004-03-15 09:45:11 +00:00]])


[6]
で、 >>4 をとりあえず実装してみましたが、そういうときに限って >>5。嫌になっちゃう。
([[名無しさん]] [WEAK[2004-03-15 09:47:45 +00:00]])
