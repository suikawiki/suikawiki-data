Integrated Architectures for Learning, Planning, and Reacting Based on Appriximating Dynamic Programming

{{paper learning/sutton90__integrated_architectures_for_learning_planning_and_reacting.pdf}}

Richard S. Sutton
{{category Richard S. Sutton}}
{{category R. S. Sutton}}
{{category S. Sutton}}
{{category R. Sutton}}

In Proceedings of the Seventh Int. Conf. on Machine Learning, pp. 216-224,
Morgan Kaufmann, 1990. {{category 1990}}

* Dyna

ロボットの意思決定のためのアーキテクチャ。
具体的には、迷路のようなところで移動する問題を取り上げている。

Dyna architerctures には Dyna-PI architecture と
Dyna-Q architecture がある。
{{category Dyna}}

* Dyna-PI

{{category Dyna-PI}}

e(x) ← e(x) + β(r + γe(y) - e(x)) = (1 - β) e(x) + β(r + γe(y))

e(x) は評価関数

状態 x の次時間ステップの評価値は、
r (報酬) と γ (蒸発率) × 評価値最大の次状態 y の和の指数移動平均


次の動作 a の選択:
P (a|x) = e^ ω_xa / Σj e^ ω_xj

ω_xa ← ω_xa + α(r + γe(y) - e(x))


** 問題

blocking problem {{category blocking problem}}:
障壁物が現れると、元の経路を使い続ける。
新たな経路もいつか見つかるかもしれないが、
時間がかかる。

shortcut problem {{category shortcut problem}}:
障壁物がなくなっても元の経路を使い続け、
新たな経路を見つけることができない。

!!! Dyna-Q

[[Q-learning]] を取り入れた。
{{category Dyna-Q}}

e(x) = max_a Q_xa

Q_xa ← Q_xa + β(r + γe(y) - Q_xa)

shortcut problem の解決のため、

Q_xa ← Q_xa + β(r + ε sqrt(n_xa) + γe(y) - Q_xa)

n_xa は前に状態 x で動作 a を選んでからの時間ステップ数。
この項を exploration bonus と呼ぶ。
不確かさを表すために入れている。

